{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **BigGAN Implmentation for Oxford 102 Flowers**"
      ],
      "metadata": {
        "id": "ZYxGktCqacte"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Imports**"
      ],
      "metadata": {
        "id": "XLjrBVCoaUFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import tarfile\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import argparse\n",
        "import types\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn import functional as F\n",
        "from torch.backends import cudnn\n",
        "from torchvision.utils import save_image\n",
        "import torchvision.datasets as dsets\n",
        "from torchvision import transforms\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from PIL import Image\n",
        "import imageio.v2 as imageio\n",
        "import re\n",
        "import matplotlib.animation as animation\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.transforms import ToPILImage\n",
        "import shutil\n",
        "from tqdm.auto import tqdm\n",
        "from typing import Union, Tuple, List, Optional"
      ],
      "metadata": {
        "id": "JxdB0A_-XZ3R"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Download Data**"
      ],
      "metadata": {
        "id": "RXX-1LgkTOQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"/content/flowers_data\", exist_ok=True)\n",
        "url = \"https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz\"\n",
        "local_path = \"/content/flowers_data/102flowers.tgz\"\n",
        "\n",
        "urllib.request.urlretrieve(url, local_path)\n",
        "\n",
        "with tarfile.open(local_path) as tar:\n",
        "    tar.extractall(path=\"/content/flowers\")\n"
      ],
      "metadata": {
        "id": "ozbIVVnhTMmn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Normalization**"
      ],
      "metadata": {
        "id": "sN1h8wUEarPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vector_l2_normalize(v: torch.Tensor, epsilon: float = 1e-12) -> torch.Tensor:\n",
        "    return v / (v.norm() + epsilon)\n",
        "\n",
        "\n",
        "class SpectralNormalization(nn.Module):\n",
        "    def __init__(self, module: nn.Module, name: str = 'weight', power_iterations: int = 1):\n",
        "        super().__init__()\n",
        "        self.module = module\n",
        "        self.name = name\n",
        "        self.power_iterations = power_iterations\n",
        "        if not hasattr(self.module, self.name + \"_u\"):\n",
        "            self._add_spectral_parameters()\n",
        "\n",
        "    def _add_spectral_parameters(self) -> None:\n",
        "        original_weight = getattr(self.module, self.name)\n",
        "        height = original_weight.data.shape[0]\n",
        "        width = original_weight.view(height, -1).data.shape[1]\n",
        "        u = Parameter(original_weight.data.new(height).normal_(0, 1), requires_grad=False)\n",
        "        v = Parameter(original_weight.data.new(width).normal_(0, 1), requires_grad=False)\n",
        "        u.data = vector_l2_normalize(u.data)\n",
        "        v.data = vector_l2_normalize(v.data)\n",
        "        w_bar = Parameter(original_weight.data)\n",
        "        del self.module._parameters[self.name]\n",
        "        self.module.register_parameter(self.name + \"_u\", u)\n",
        "        self.module.register_parameter(self.name + \"_v\", v)\n",
        "        self.module.register_parameter(self.name + \"_bar\", w_bar)\n",
        "\n",
        "    def _update_uv_vectors(self) -> None:\n",
        "        u = getattr(self.module, self.name + \"_u\")\n",
        "        v = getattr(self.module, self.name + \"_v\")\n",
        "        w_bar = getattr(self.module, self.name + \"_bar\")\n",
        "        height = w_bar.data.shape[0]\n",
        "        w_matrix = w_bar.view(height, -1).data\n",
        "        for _ in range(self.power_iterations):\n",
        "            v.data = vector_l2_normalize(torch.mv(w_matrix.T, u.data))\n",
        "            u.data = vector_l2_normalize(torch.mv(w_matrix, v.data))\n",
        "        sigma = torch.dot(u.data, torch.mv(w_matrix, v.data))\n",
        "        setattr(self.module, self.name, w_bar / sigma.expand_as(w_bar))\n",
        "\n",
        "    def forward(self, *args) -> torch.Tensor:\n",
        "        self._update_uv_vectors()\n",
        "        return self.module.forward(*args)\n",
        "\n",
        "\n",
        "class AdaptiveNormalization(nn.Module):\n",
        "    def __init__(self, channels: int, condition_dim: int = 148):\n",
        "        super().__init__()\n",
        "        self.batch_norm = nn.BatchNorm2d(channels, affine=False)\n",
        "        self.embedding_layer = nn.Linear(condition_dim, channels * 2)\n",
        "        nn.init.constant_(self.embedding_layer.weight.data[:, :channels], 1.0)\n",
        "        nn.init.constant_(self.embedding_layer.weight.data[:, channels:], 0.0)\n",
        "        if self.embedding_layer.bias is not None:\n",
        "            nn.init.constant_(self.embedding_layer.bias.data[:channels], 1.0)\n",
        "            nn.init.constant_(self.embedding_layer.bias.data[channels:], 0.0)\n",
        "\n",
        "    def forward(self, input_tensor: torch.Tensor, condition_vector: torch.Tensor) -> torch.Tensor:\n",
        "        normalized_output = self.batch_norm(input_tensor)\n",
        "        gamma_beta = self.embedding_layer(condition_vector)\n",
        "        gamma, beta = gamma_beta.chunk(2, 1)\n",
        "        gamma = gamma.unsqueeze(2).unsqueeze(3)\n",
        "        beta = beta.unsqueeze(2).unsqueeze(3)\n",
        "        output = gamma * normalized_output + beta\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "vJMuYabzZvVW"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Attention Module**"
      ],
      "metadata": {
        "id": "9MIiowX4bOgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SpatialAttentionBlock(nn.Module):\n",
        "    def __init__(self, in_channels: int):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.query_conv = SpectralNormalization(nn.Conv2d(in_channels, in_channels // 8, kernel_size=1))\n",
        "        self.key_conv = SpectralNormalization(nn.Conv2d(in_channels, in_channels // 8, kernel_size=1))\n",
        "        self.value_conv = SpectralNormalization(nn.Conv2d(in_channels, in_channels, kernel_size=1))\n",
        "        self.gamma_param = nn.Parameter(torch.zeros(1))\n",
        "        self.softmax_fn = nn.Softmax(dim=-1)\n",
        "        self.post_attention_conv = SpectralNormalization(nn.Conv2d(in_channels, in_channels, kernel_size=1))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        batch_size, C, H, W = x.size()\n",
        "        proj_query = self.query_conv(x).view(batch_size, -1, H * W).permute(0, 2, 1)\n",
        "        proj_key = self.key_conv(x).view(batch_size, -1, H * W)\n",
        "        proj_value = self.value_conv(x).view(batch_size, -1, H * W)\n",
        "        energy = torch.bmm(proj_query, proj_key)\n",
        "        attention = self.softmax_fn(energy)\n",
        "        output_attention = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
        "        output_attention = output_attention.view(batch_size, C, H, W)\n",
        "        output_attention = self.post_attention_conv(output_attention)\n",
        "        output = self.gamma_param * output_attention + x\n",
        "        return output\n",
        "\n"
      ],
      "metadata": {
        "id": "mHZno6VXZncb"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Residual Blocks**"
      ],
      "metadata": {
        "id": "k3S5W6eUbB4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GenResBlock(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int = 3, padding: int = 1,\n",
        "                 condition_dim: int = 148, use_upsample: bool = True):\n",
        "        super().__init__()\n",
        "        self.use_upsample = use_upsample\n",
        "        self.norm1 = AdaptiveNormalization(in_channels, condition_dim)\n",
        "        self.norm2 = AdaptiveNormalization(out_channels, condition_dim)\n",
        "        self.conv1 = SpectralNormalization(nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding))\n",
        "        self.conv2 = SpectralNormalization(nn.Conv2d(out_channels, out_channels, kernel_size, padding=padding))\n",
        "        self.conv3 = SpectralNormalization(nn.Conv2d(out_channels, out_channels, kernel_size, padding=padding))\n",
        "        self.activation = nn.ReLU()\n",
        "        self.skip_projection = False\n",
        "        if in_channels != out_channels or use_upsample:\n",
        "            self.conv_skip = SpectralNormalization(nn.Conv2d(in_channels, out_channels, 1, padding=0))\n",
        "            self.skip_projection = True\n",
        "\n",
        "    def forward(self, input_tensor: torch.Tensor, condition_vector: torch.Tensor) -> torch.Tensor:\n",
        "        main_path_features = self.norm1(input_tensor, condition_vector)\n",
        "        main_path_features = self.activation(main_path_features)\n",
        "        if self.use_upsample:\n",
        "            main_path_features = F.interpolate(main_path_features, scale_factor=2, mode='nearest')\n",
        "        main_path_features = self.conv1(main_path_features)\n",
        "        main_path_features = self.norm2(main_path_features, condition_vector)\n",
        "        main_path_features = self.activation(main_path_features)\n",
        "        main_path_features = self.conv2(main_path_features)\n",
        "        main_path_features = self.conv3(main_path_features)\n",
        "        skip_features = input_tensor\n",
        "        if self.skip_projection:\n",
        "            skip_features = self.activation(skip_features)\n",
        "            if self.use_upsample:\n",
        "                skip_features = F.interpolate(skip_features, scale_factor=2, mode='nearest')\n",
        "            skip_features = self.conv_skip(skip_features)\n",
        "        return main_path_features + skip_features\n",
        "\n",
        "\n",
        "class DiscResBlock(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int = 3, padding: int = 1,\n",
        "                 use_downsample: bool = True):\n",
        "        super().__init__()\n",
        "        self.use_downsample = use_downsample\n",
        "        self.conv1 = SpectralNormalization(nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding))\n",
        "        self.conv2 = SpectralNormalization(nn.Conv2d(out_channels, out_channels, kernel_size, padding=padding))\n",
        "        self.conv3 = SpectralNormalization(nn.Conv2d(out_channels, out_channels, kernel_size, padding=padding))\n",
        "        self.activation = nn.LeakyReLU(0.2)\n",
        "        self.skip_projection = False\n",
        "        if in_channels != out_channels or use_downsample:\n",
        "            self.conv_skip = SpectralNormalization(nn.Conv2d(in_channels, out_channels, 1, padding=0))\n",
        "            self.skip_projection = True\n",
        "\n",
        "    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
        "        main_path_features = self.conv1(input_tensor)\n",
        "        main_path_features = self.activation(main_path_features)\n",
        "        if self.use_downsample:\n",
        "            main_path_features = F.avg_pool2d(main_path_features, 2)\n",
        "        main_path_features = self.conv2(main_path_features)\n",
        "        main_path_features = self.activation(main_path_features)\n",
        "        main_path_features = self.conv3(main_path_features)\n",
        "        skip_features = input_tensor\n",
        "        if self.skip_projection:\n",
        "            skip_features = self.activation(skip_features)\n",
        "            skip_features = self.conv_skip(skip_features)\n",
        "            if self.use_downsample:\n",
        "                skip_features = F.avg_pool2d(skip_features, 2)\n",
        "        return main_path_features + skip_features\n",
        "\n"
      ],
      "metadata": {
        "id": "0RihAv3Ea8I3"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Generator and Discriminator Model**"
      ],
      "metadata": {
        "id": "Q22L0WeCbGR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageGenerator(nn.Module):\n",
        "    def __init__(self, latent_dim: int = 120, num_classes: int = 1000, base_channels: int = 96):\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_classes = num_classes\n",
        "        self.base_channels = base_channels\n",
        "\n",
        "        self.class_embedding_layer = SpectralNormalization(nn.Linear(self.num_classes, 128, bias=False))\n",
        "\n",
        "        initial_feature_map_res = 8\n",
        "        self.initial_feature_map_channels = 16 * self.base_channels\n",
        "\n",
        "        num_gen_res_blocks_for_latent_split = 4\n",
        "        self.latent_chunk_size = self.latent_dim // (1 + num_gen_res_blocks_for_latent_split)\n",
        "\n",
        "        # FIX APPLIED HERE: The input dimension of nn.Linear now uses self.latent_chunk_size\n",
        "        self.initial_dense_layer = SpectralNormalization(\n",
        "            nn.Linear(self.latent_chunk_size, initial_feature_map_res * initial_feature_map_res * self.initial_feature_map_channels)\n",
        "        )\n",
        "\n",
        "        self.gen_blocks = nn.ModuleList([\n",
        "            GenResBlock(16 * self.base_channels, 8 * self.base_channels, condition_dim=self.latent_chunk_size + 128, use_upsample=True),\n",
        "            GenResBlock(8 * self.base_channels, 4 * self.base_channels, condition_dim=self.latent_chunk_size + 128, use_upsample=True),\n",
        "            SpatialAttentionBlock(4 * self.base_channels),\n",
        "            GenResBlock(4 * self.base_channels, 2 * self.base_channels, condition_dim=self.latent_chunk_size + 128, use_upsample=True),\n",
        "            GenResBlock(2 * self.base_channels, 1 * self.base_channels, condition_dim=self.latent_chunk_size + 128, use_upsample=True)\n",
        "        ])\n",
        "\n",
        "        self.final_batch_norm = nn.BatchNorm2d(1 * self.base_channels)\n",
        "        self.output_conv_pre_tanh = SpectralNormalization(nn.Conv2d(1 * self.base_channels, 3, kernel_size=3, padding=1))\n",
        "\n",
        "    def forward(self, latent_code: torch.Tensor, class_one_hot: torch.Tensor) -> torch.Tensor:\n",
        "        latent_code_parts = torch.split(latent_code, self.latent_chunk_size, dim=1)\n",
        "        class_embedding = self.class_embedding_layer(class_one_hot)\n",
        "        out = self.initial_dense_layer(latent_code_parts[0])\n",
        "        out = out.view(-1, self.initial_feature_map_channels, 8, 8)\n",
        "        latent_part_idx = 1\n",
        "        for block in self.gen_blocks:\n",
        "            if isinstance(block, GenResBlock):\n",
        "                condition_vector = torch.cat([latent_code_parts[latent_part_idx], class_embedding], dim=1)\n",
        "                out = block(out, condition_vector)\n",
        "                latent_part_idx += 1\n",
        "            else:\n",
        "                out = block(out)\n",
        "        out = self.final_batch_norm(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.output_conv_pre_tanh(out)\n",
        "        return torch.tanh(out)\n",
        "\n",
        "\n",
        "class ImageDiscriminator(nn.Module):\n",
        "    def __init__(self, num_classes: int = 1000, base_channels: int = 96):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.base_channels = base_channels\n",
        "\n",
        "        self.initial_block = nn.Sequential(\n",
        "            SpectralNormalization(nn.Conv2d(3, 1 * self.base_channels, kernel_size=3, padding=1)),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            SpectralNormalization(nn.Conv2d(1 * self.base_channels, 1 * self.base_channels, kernel_size=3, padding=1)),\n",
        "            nn.AvgPool2d(2)\n",
        "        )\n",
        "        self.initial_skip_conv = SpectralNormalization(nn.Conv2d(3, 1 * self.base_channels, kernel_size=1, padding=0))\n",
        "\n",
        "        self.disc_blocks = nn.Sequential(\n",
        "            DiscResBlock(1 * self.base_channels, 1 * self.base_channels, use_downsample=True),\n",
        "            DiscResBlock(1 * self.base_channels, 2 * self.base_channels, use_downsample=True),\n",
        "            SpatialAttentionBlock(2 * self.base_channels),\n",
        "            DiscResBlock(2 * self.base_channels, 4 * self.base_channels, use_downsample=True),\n",
        "            DiscResBlock(4 * self.base_channels, 8 * self.base_channels, use_downsample=True),\n",
        "            DiscResBlock(8 * self.base_channels, 16 * self.base_channels, use_downsample=True),\n",
        "            DiscResBlock(16 * self.base_channels, 16 * self.base_channels, use_downsample=False)\n",
        "        )\n",
        "\n",
        "        self.output_linear = SpectralNormalization(nn.Linear(16 * self.base_channels, 1))\n",
        "        self.class_embedding_layer = SpectralNormalization(nn.Embedding(self.num_classes, 16 * self.base_channels))\n",
        "        self.class_embedding_layer.module.weight_bar.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "        self.activation = nn.LeakyReLU(0.2)\n",
        "\n",
        "    def forward(self, image: torch.Tensor, class_id_int: torch.Tensor) -> torch.Tensor:\n",
        "        out = self.initial_block(image)\n",
        "        out = out + self.initial_skip_conv(F.avg_pool2d(image, 2))\n",
        "        out = self.activation(out)\n",
        "        out = self.disc_blocks(out)\n",
        "        out = self.activation(out)\n",
        "        out = F.adaptive_avg_pool2d(out, 1).view(out.size(0), -1)\n",
        "        real_fake_score = self.output_linear(out).squeeze(1)\n",
        "        class_embedding = self.class_embedding_layer(class_id_int)\n",
        "        class_conditional_score = (self.activation(out) * class_embedding).sum(1)\n",
        "        return real_fake_score + class_conditional_score"
      ],
      "metadata": {
        "id": "Q2VKYJiLZakw"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model Architecture**"
      ],
      "metadata": {
        "id": "Al3xq1PL9Khy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "G = ImageGenerator(latent_dim=120, num_classes=1, base_channels=64).to('cuda')\n",
        "D = ImageDiscriminator(num_classes=1, base_channels=64).to('cuda')\n",
        "\n",
        "\n",
        "print(\"\\n--- Generator Architecture ---\")\n",
        "print(G)\n",
        "print(\"\\n--- Discriminator Architecture ---\")\n",
        "print(D)\n",
        "\n",
        "del G\n",
        "del D\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "8CPs9Luo8AXC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0975bec0-6a8d-49d6-9e18-0e15dffb31d3"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Generator Architecture ---\n",
            "ImageGenerator(\n",
            "  (class_embedding_layer): SpectralNormalization(\n",
            "    (module): Linear(in_features=1, out_features=128, bias=False)\n",
            "  )\n",
            "  (initial_dense_layer): SpectralNormalization(\n",
            "    (module): Linear(in_features=24, out_features=65536, bias=True)\n",
            "  )\n",
            "  (gen_blocks): ModuleList(\n",
            "    (0): GenResBlock(\n",
            "      (norm1): AdaptiveNormalization(\n",
            "        (batch_norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
            "        (embedding_layer): Linear(in_features=152, out_features=2048, bias=True)\n",
            "      )\n",
            "      (norm2): AdaptiveNormalization(\n",
            "        (batch_norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
            "        (embedding_layer): Linear(in_features=152, out_features=1024, bias=True)\n",
            "      )\n",
            "      (conv1): SpectralNormalization(\n",
            "        (module): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv2): SpectralNormalization(\n",
            "        (module): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv3): SpectralNormalization(\n",
            "        (module): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (activation): ReLU()\n",
            "      (conv_skip): SpectralNormalization(\n",
            "        (module): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (1): GenResBlock(\n",
            "      (norm1): AdaptiveNormalization(\n",
            "        (batch_norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
            "        (embedding_layer): Linear(in_features=152, out_features=1024, bias=True)\n",
            "      )\n",
            "      (norm2): AdaptiveNormalization(\n",
            "        (batch_norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
            "        (embedding_layer): Linear(in_features=152, out_features=512, bias=True)\n",
            "      )\n",
            "      (conv1): SpectralNormalization(\n",
            "        (module): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv2): SpectralNormalization(\n",
            "        (module): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv3): SpectralNormalization(\n",
            "        (module): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (activation): ReLU()\n",
            "      (conv_skip): SpectralNormalization(\n",
            "        (module): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (2): SpatialAttentionBlock(\n",
            "      (query_conv): SpectralNormalization(\n",
            "        (module): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (key_conv): SpectralNormalization(\n",
            "        (module): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (value_conv): SpectralNormalization(\n",
            "        (module): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (softmax_fn): Softmax(dim=-1)\n",
            "      (post_attention_conv): SpectralNormalization(\n",
            "        (module): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (3): GenResBlock(\n",
            "      (norm1): AdaptiveNormalization(\n",
            "        (batch_norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
            "        (embedding_layer): Linear(in_features=152, out_features=512, bias=True)\n",
            "      )\n",
            "      (norm2): AdaptiveNormalization(\n",
            "        (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
            "        (embedding_layer): Linear(in_features=152, out_features=256, bias=True)\n",
            "      )\n",
            "      (conv1): SpectralNormalization(\n",
            "        (module): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv2): SpectralNormalization(\n",
            "        (module): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv3): SpectralNormalization(\n",
            "        (module): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (activation): ReLU()\n",
            "      (conv_skip): SpectralNormalization(\n",
            "        (module): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (4): GenResBlock(\n",
            "      (norm1): AdaptiveNormalization(\n",
            "        (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
            "        (embedding_layer): Linear(in_features=152, out_features=256, bias=True)\n",
            "      )\n",
            "      (norm2): AdaptiveNormalization(\n",
            "        (batch_norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
            "        (embedding_layer): Linear(in_features=152, out_features=128, bias=True)\n",
            "      )\n",
            "      (conv1): SpectralNormalization(\n",
            "        (module): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv2): SpectralNormalization(\n",
            "        (module): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv3): SpectralNormalization(\n",
            "        (module): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (activation): ReLU()\n",
            "      (conv_skip): SpectralNormalization(\n",
            "        (module): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (final_batch_norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (output_conv_pre_tanh): SpectralNormalization(\n",
            "    (module): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            ")\n",
            "\n",
            "--- Discriminator Architecture ---\n",
            "ImageDiscriminator(\n",
            "  (initial_block): Sequential(\n",
            "    (0): SpectralNormalization(\n",
            "      (module): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    )\n",
            "    (1): LeakyReLU(negative_slope=0.2)\n",
            "    (2): SpectralNormalization(\n",
            "      (module): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    )\n",
            "    (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "  )\n",
            "  (initial_skip_conv): SpectralNormalization(\n",
            "    (module): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (disc_blocks): Sequential(\n",
            "    (0): DiscResBlock(\n",
            "      (conv1): SpectralNormalization(\n",
            "        (module): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv2): SpectralNormalization(\n",
            "        (module): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv3): SpectralNormalization(\n",
            "        (module): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (activation): LeakyReLU(negative_slope=0.2)\n",
            "      (conv_skip): SpectralNormalization(\n",
            "        (module): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (1): DiscResBlock(\n",
            "      (conv1): SpectralNormalization(\n",
            "        (module): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv2): SpectralNormalization(\n",
            "        (module): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv3): SpectralNormalization(\n",
            "        (module): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (activation): LeakyReLU(negative_slope=0.2)\n",
            "      (conv_skip): SpectralNormalization(\n",
            "        (module): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (2): SpatialAttentionBlock(\n",
            "      (query_conv): SpectralNormalization(\n",
            "        (module): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (key_conv): SpectralNormalization(\n",
            "        (module): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (value_conv): SpectralNormalization(\n",
            "        (module): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (softmax_fn): Softmax(dim=-1)\n",
            "      (post_attention_conv): SpectralNormalization(\n",
            "        (module): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (3): DiscResBlock(\n",
            "      (conv1): SpectralNormalization(\n",
            "        (module): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv2): SpectralNormalization(\n",
            "        (module): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv3): SpectralNormalization(\n",
            "        (module): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (activation): LeakyReLU(negative_slope=0.2)\n",
            "      (conv_skip): SpectralNormalization(\n",
            "        (module): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (4): DiscResBlock(\n",
            "      (conv1): SpectralNormalization(\n",
            "        (module): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv2): SpectralNormalization(\n",
            "        (module): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv3): SpectralNormalization(\n",
            "        (module): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (activation): LeakyReLU(negative_slope=0.2)\n",
            "      (conv_skip): SpectralNormalization(\n",
            "        (module): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (5): DiscResBlock(\n",
            "      (conv1): SpectralNormalization(\n",
            "        (module): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv2): SpectralNormalization(\n",
            "        (module): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv3): SpectralNormalization(\n",
            "        (module): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (activation): LeakyReLU(negative_slope=0.2)\n",
            "      (conv_skip): SpectralNormalization(\n",
            "        (module): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (6): DiscResBlock(\n",
            "      (conv1): SpectralNormalization(\n",
            "        (module): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv2): SpectralNormalization(\n",
            "        (module): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv3): SpectralNormalization(\n",
            "        (module): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (activation): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "  )\n",
            "  (output_linear): SpectralNormalization(\n",
            "    (module): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  )\n",
            "  (class_embedding_layer): SpectralNormalization(\n",
            "    (module): Embedding(1, 1024)\n",
            "  )\n",
            "  (activation): LeakyReLU(negative_slope=0.2)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Training the model**"
      ],
      "metadata": {
        "id": "jPrnRr2VbfxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imsize = 128\n",
        "z_dim = 120\n",
        "chn = 64 # Base channels for Generator and Discriminator\n",
        "lambda_gp = 10.0 # Gradient penalty weight\n",
        "version = 'Gan_flower102' # Version string for output directories\n",
        "total_step = 200000 # Total training steps\n",
        "d_iters = 5 # Discriminator iterations per generator iteration\n",
        "batch_size = 32\n",
        "num_workers = 2\n",
        "g_lr = 0.0001 # Generator learning rate\n",
        "d_lr = 0.0004 # Discriminator learning rate\n",
        "beta1 = 0.0 # Adam beta1\n",
        "beta2 = 0.9 # Adam beta2\n",
        "seed = 42 # Random seed\n",
        "image_path = '/content/flowers' # Path to extracted flower images\n",
        "log_path = '/content/logs' # Path for TensorBoard logs\n",
        "model_save_path = '/content/models' # Path for saving model\n",
        "sample_path = '/content/samples' # Path for saving generated samples\n",
        "attn_path = '/content/attn' # Path for attention maps (not used in this version)\n",
        "\n",
        "log_step = 100 # Frequency for logging training progress\n",
        "sample_step = 5000 # Frequency for saving generated samples\n",
        "\n",
        "pretrained_gen_path = '100000_G.pth'\n",
        "pretrained_disc_path = '100000_D.pth'\n",
        "\n",
        "#pretrained_gen_path = None\n",
        "#pretrained_disc_path = None\n",
        "\n",
        "# --- Global Setup and Directory Creation ---\n",
        "cudnn.benchmark = True # Optimize cuDNN for faster training\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True # Ensure reproducibility\n",
        "\n",
        "# --- Helper Functions for Training Loop ---\n",
        "\n",
        "def create_output_directory(base_path: str, sub_version: str = \"\") -> str:\n",
        "    full_path = os.path.join(base_path, sub_version) if sub_version else base_path\n",
        "    os.makedirs(full_path, exist_ok=True)\n",
        "    return full_path\n",
        "\n",
        "def move_tensor_to_device(data: torch.Tensor, device: torch.device) -> torch.Tensor:\n",
        "    return data.to(device)\n",
        "\n",
        "def denormalize_image(x: torch.Tensor) -> torch.Tensor:\n",
        "    return ((x + 1) / 2).clamp_(0, 1)\n",
        "\n",
        "def zero_grad_optimizers():\n",
        "    disc_optimizer.zero_grad()\n",
        "    gen_optimizer.zero_grad()\n",
        "\n",
        "def generate_random_labels(batch_size_val, num_classes_val, device_val):\n",
        "    labels_int = torch.randint(0, num_classes_val, (batch_size_val,)).to(device_val)\n",
        "    labels_one_hot = F.one_hot(labels_int, num_classes=num_classes_val).float().to(device_val)\n",
        "    return labels_int, labels_one_hot\n",
        "\n",
        "def save_real_image_sample_func(data_loader_val, sample_output_dir_val):\n",
        "    real_images, _ = next(iter(data_loader_val))\n",
        "    save_image(denormalize_image(real_images), os.path.join(sample_output_dir_val, 'real_images_sample.png'))\n",
        "\n",
        "# Create output directories\n",
        "model_output_dir = create_output_directory(model_save_path, version)\n",
        "sample_output_dir = create_output_directory(sample_path, version)\n",
        "log_output_dir = create_output_directory(log_path, version)\n",
        "\n",
        "# Determine device for training\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Training on device: {device}\")\n",
        "\n",
        "\n",
        "# --- Data Loader Initialization ---\n",
        "# Define image transformations\n",
        "transform_list = [\n",
        "    transforms.CenterCrop(160), # Crop to a square\n",
        "    transforms.Resize((imsize, imsize)), # Resize to target size\n",
        "    transforms.ToTensor(), # Convert to tensor\n",
        "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)) # Normalize to [-1, 1]\n",
        "]\n",
        "image_transforms = transforms.Compose(transform_list)\n",
        "\n",
        "# Load dataset from image folder structure\n",
        "dataset = dsets.ImageFolder(image_path, transform=image_transforms)\n",
        "num_classes = len(dataset.classes) # Get number of classes\n",
        "\n",
        "# Create data loader\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset=dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=True # Drop the last incomplete batch\n",
        ")\n",
        "\n",
        "\n",
        "# --- Model and Optimizer Initialization---\n",
        "generator = ImageGenerator(\n",
        "    latent_dim=z_dim,\n",
        "    num_classes=num_classes,\n",
        "    base_channels=chn\n",
        ").to(device)\n",
        "\n",
        "discriminator = ImageDiscriminator(\n",
        "    num_classes=num_classes,\n",
        "    base_channels=chn\n",
        ").to(device)\n",
        "\n",
        "gen_optimizer = torch.optim.Adam(\n",
        "    filter(lambda p: p.requires_grad, generator.parameters()),\n",
        "    lr=g_lr, betas=[beta1, beta2]\n",
        ")\n",
        "disc_optimizer = torch.optim.Adam(\n",
        "    filter(lambda p: p.requires_grad, discriminator.parameters()),\n",
        "    lr=d_lr, betas=[beta1, beta2]\n",
        ")\n",
        "\n",
        "# --- Load Pre-trained Models ---\n",
        "start_step = 0\n",
        "if pretrained_gen_path:\n",
        "    gen_load_path = os.path.join(model_output_dir, pretrained_gen_path)\n",
        "    if os.path.exists(gen_load_path):\n",
        "        generator.load_state_dict(torch.load(gen_load_path, map_location=device))\n",
        "        match = re.search(r'(\\d+)_G\\.pth', pretrained_gen_path)\n",
        "        start_step = int(match.group(1))\n",
        "\n",
        "if pretrained_disc_path:\n",
        "    disc_load_path = os.path.join(model_output_dir, pretrained_disc_path)\n",
        "    if os.path.exists(disc_load_path):\n",
        "        discriminator.load_state_dict(torch.load(disc_load_path, map_location=device))\n",
        "\n",
        "# --- TensorBoard Setup ---\n",
        "tf_log_path = os.path.join(log_output_dir, 'tensorboard_logs')\n",
        "summary_writer = SummaryWriter(log_dir=tf_log_path)\n",
        "\n",
        "# Save a sample of real images\n",
        "save_real_image_sample_func(data_loader, sample_output_dir)\n",
        "\n",
        "\n",
        "# --- Training Loop ---\n",
        "data_iterator = iter(data_loader)\n",
        "\n",
        "# Fixed latent code and labels for consistent sample generation\n",
        "fixed_latent_z = move_tensor_to_device(torch.randn(batch_size, z_dim), device)\n",
        "fixed_labels_int, fixed_labels_one_hot = generate_random_labels(batch_size, num_classes, device)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# History lists for plotting losses\n",
        "d_total_loss_history = []\n",
        "g_total_loss_history = []\n",
        "steps_history = []\n",
        "\n",
        "print('Starting GAN training...')\n",
        "for step in range(start_step, total_step):\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "\n",
        "    # --- Train Discriminator ---\n",
        "    # Fetch real images and labels\n",
        "    try:\n",
        "        real_images, real_labels_int = next(data_iterator)\n",
        "    except StopIteration:\n",
        "        # Reset iterator if end of dataset is reached\n",
        "        data_iterator = iter(data_loader)\n",
        "        real_images, real_labels_int = next(data_iterator)\n",
        "\n",
        "    real_images = move_tensor_to_device(real_images, device)\n",
        "    real_labels_int = move_tensor_to_device(real_labels_int, device)\n",
        "\n",
        "    # Discriminator output for real images\n",
        "    d_out_real = discriminator(real_images, real_labels_int)\n",
        "    d_loss_real = - torch.mean(d_out_real) # Maximize D(real)\n",
        "\n",
        "    # Generate fake images\n",
        "    z_latent = move_tensor_to_device(torch.randn(batch_size, z_dim), device)\n",
        "    fake_labels_int, fake_labels_one_hot = generate_random_labels(batch_size, num_classes, device)\n",
        "    fake_images = generator(z_latent, fake_labels_one_hot)\n",
        "\n",
        "    # Discriminator output for fake images\n",
        "    d_out_fake = discriminator(fake_images.detach(), fake_labels_int) # Detach fake_images to prevent G from being updated\n",
        "    d_loss_fake = d_out_fake.mean() # Minimize D(fake)\n",
        "\n",
        "    # Calculate Gradient Penalty\n",
        "    # Interpolate between real and fake images\n",
        "    alpha = torch.rand(real_images.size(0), 1, 1, 1, device=device)\n",
        "    alpha = alpha.expand_as(real_images)\n",
        "    interpolated_images = (alpha * real_images.data + (1 - alpha) * fake_images.data).requires_grad_(True)\n",
        "\n",
        "    # Discriminator output for interpolated images\n",
        "    d_out_interpolated = discriminator(interpolated_images, real_labels_int)\n",
        "\n",
        "    # Compute gradients of D_out_interpolated with respect to interpolated_images\n",
        "    gradients = torch.autograd.grad(\n",
        "        outputs=d_out_interpolated,\n",
        "        inputs=interpolated_images,\n",
        "        grad_outputs=torch.ones_like(d_out_interpolated, device=device),\n",
        "        retain_graph=True,\n",
        "        create_graph=True,\n",
        "        only_inputs=True\n",
        "    )[0]\n",
        "\n",
        "    # Calculate gradient norm and penalty\n",
        "    gradients = gradients.view(gradients.size(0), -1)\n",
        "    grad_norm = gradients.norm(2, dim=1)\n",
        "    d_loss_gp = torch.mean((grad_norm - 1) ** 2)\n",
        "\n",
        "    # Total Discriminator Loss\n",
        "    total_d_loss = d_loss_real + d_loss_fake + lambda_gp * d_loss_gp\n",
        "\n",
        "    # Backward pass and optimize Discriminator\n",
        "    zero_grad_optimizers()\n",
        "    total_d_loss.backward()\n",
        "    disc_optimizer.step()\n",
        "\n",
        "    # --- Train Generator (every d_iters steps) ---\n",
        "    if (step + 1) % d_iters == 0:\n",
        "        # Generate new fake images\n",
        "        z_latent = move_tensor_to_device(torch.randn(batch_size, z_dim), device)\n",
        "        fake_labels_int, fake_labels_one_hot = generate_random_labels(batch_size, num_classes, device)\n",
        "\n",
        "        fake_images = generator(z_latent, fake_labels_one_hot)\n",
        "        g_out_fake = discriminator(fake_images, fake_labels_int) # D(G(z))\n",
        "\n",
        "        gen_loss = - g_out_fake.mean() # Maximize D(G(z))\n",
        "\n",
        "        # Backward pass and optimize Generator\n",
        "        zero_grad_optimizers()\n",
        "        gen_loss.backward()\n",
        "        gen_optimizer.step()\n",
        "\n",
        "        # --- Logging and Monitoring ---\n",
        "        if (step + 1) % log_step == 0:\n",
        "            elapsed_time = time.time() - start_time\n",
        "            elapsed_time_str = str(datetime.timedelta(seconds=elapsed_time))\n",
        "            log_message = (\n",
        "                f\"Step [{step + 1}/{total_step}], \"\n",
        "                f\"D_loss_real: {d_loss_real.item():.4f}, D_loss_fake: {d_loss_fake.item():.4f}, \"\n",
        "            )\n",
        "            log_message += f\"D_loss_gp: {d_loss_gp.item():.4f}, \"\n",
        "            log_message += f\"G_loss: {gen_loss.item():.4f}\"\n",
        "            print(log_message)\n",
        "\n",
        "            steps_history.append(step)\n",
        "            d_total_loss_history.append(total_d_loss.item())\n",
        "            g_total_loss_history.append(gen_loss.item())\n",
        "\n",
        "            # Log to TensorBoard\n",
        "            summary_writer.add_scalar('Loss/D_real', d_loss_real.item(), (step + 1))\n",
        "            summary_writer.add_scalar('Loss/D_fake', d_loss_fake.item(), (step + 1))\n",
        "            summary_writer.add_scalar('Loss/D_total', total_d_loss.item(), (step + 1))\n",
        "            summary_writer.add_scalar('Loss/D_gp', d_loss_gp.item(), (step + 1))\n",
        "            summary_writer.add_scalar('Loss/G_total', gen_loss.item(), (step + 1))\n",
        "\n",
        "        # --- Save Sample Images ---\n",
        "        if (step + 1) % sample_step == 0:\n",
        "            generator.eval() # Set generator to evaluation mode\n",
        "            with torch.no_grad(): # Disable gradient calculation\n",
        "                generated_samples = generator(fixed_latent_z, fixed_labels_one_hot)\n",
        "            save_image(denormalize_image(generated_samples.data),\n",
        "                       os.path.join(sample_output_dir, f'{step + 1}_generated.png'))\n",
        "            generator.train() # Set generator back to training mode\n",
        "\n",
        "# Save final models after training completes\n",
        "torch.save(generator.state_dict(), os.path.join(model_output_dir, f'{total_step}_G.pth'))\n",
        "torch.save(discriminator.state_dict(), os.path.join(model_output_dir, f'{total_step}_D.pth'))\n",
        "\n",
        "\n",
        "# Close TensorBoard writer\n",
        "summary_writer.close()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ehkpeyQvYNpQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfc6cbaf-da2a-46fe-99f5-1f5efe547940"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on device: cuda\n",
            "Starting GAN training...\n",
            "Step [100/200000], D_loss_real: -39.2209, D_loss_fake: 37.9270, D_loss_gp: 0.8547, G_loss: -74.5643\n",
            "Step [200/200000], D_loss_real: -145.5524, D_loss_fake: 139.3123, D_loss_gp: 0.8320, G_loss: -251.2175\n",
            "Step [300/200000], D_loss_real: -13697.8721, D_loss_fake: 12554.5645, D_loss_gp: 38.6093, G_loss: -45972.8125\n",
            "Step [400/200000], D_loss_real: -4251.8418, D_loss_fake: 4213.4360, D_loss_gp: 0.8044, G_loss: -4543.2266\n",
            "Step [500/200000], D_loss_real: -984.1354, D_loss_fake: 897.4501, D_loss_gp: 0.5573, G_loss: -1889.8650\n",
            "Step [600/200000], D_loss_real: -12712.3906, D_loss_fake: 12653.5352, D_loss_gp: 2.6796, G_loss: -12179.7793\n",
            "Step [700/200000], D_loss_real: -441.6573, D_loss_fake: 437.8347, D_loss_gp: 0.6985, G_loss: -651.1124\n",
            "Step [800/200000], D_loss_real: -7652.1240, D_loss_fake: 7625.7651, D_loss_gp: 1.7900, G_loss: -7534.3955\n",
            "Step [900/200000], D_loss_real: -8183.1973, D_loss_fake: 8157.1323, D_loss_gp: 0.7101, G_loss: -8150.9258\n",
            "Step [1000/200000], D_loss_real: -7089.1338, D_loss_fake: 7041.4248, D_loss_gp: 0.7160, G_loss: -7173.1138\n",
            "Step [1100/200000], D_loss_real: -7531.4727, D_loss_fake: 7494.3779, D_loss_gp: 0.2431, G_loss: -9172.8828\n",
            "Step [1200/200000], D_loss_real: -7584.5381, D_loss_fake: 7551.8643, D_loss_gp: 0.5122, G_loss: -7910.2324\n",
            "Step [1300/200000], D_loss_real: -6304.5342, D_loss_fake: 6267.8955, D_loss_gp: 1.3007, G_loss: -5156.5024\n",
            "Step [1400/200000], D_loss_real: -3146.0513, D_loss_fake: 3121.6355, D_loss_gp: 0.1153, G_loss: -4123.9077\n",
            "Step [1500/200000], D_loss_real: -7125.3843, D_loss_fake: 7042.5820, D_loss_gp: 8.0926, G_loss: -4822.5996\n",
            "Step [1600/200000], D_loss_real: -6161.6416, D_loss_fake: 6122.6030, D_loss_gp: 1.9399, G_loss: -4923.2002\n",
            "Step [1700/200000], D_loss_real: -4334.9795, D_loss_fake: 4307.5078, D_loss_gp: 0.3186, G_loss: -4817.5713\n",
            "Step [1800/200000], D_loss_real: -3913.4958, D_loss_fake: 3893.6589, D_loss_gp: 0.1776, G_loss: -4027.9211\n",
            "Step [1900/200000], D_loss_real: -5511.8428, D_loss_fake: 5459.3291, D_loss_gp: 1.9119, G_loss: -4535.1069\n",
            "Step [2000/200000], D_loss_real: -4007.5015, D_loss_fake: 3992.3203, D_loss_gp: 0.3665, G_loss: -3877.2078\n",
            "Step [2100/200000], D_loss_real: -4333.6592, D_loss_fake: 4301.0889, D_loss_gp: 0.6518, G_loss: -4289.4561\n",
            "Step [2200/200000], D_loss_real: -4399.3364, D_loss_fake: 4373.3970, D_loss_gp: 0.7831, G_loss: -3904.9556\n",
            "Step [2300/200000], D_loss_real: -4164.0098, D_loss_fake: 4148.7573, D_loss_gp: 0.2853, G_loss: -3911.8408\n",
            "Step [2400/200000], D_loss_real: -4440.9961, D_loss_fake: 4405.4663, D_loss_gp: 0.6805, G_loss: -4718.3682\n",
            "Step [2500/200000], D_loss_real: -4589.2764, D_loss_fake: 4561.8696, D_loss_gp: 0.3957, G_loss: -4974.5396\n",
            "Step [2600/200000], D_loss_real: -4636.6357, D_loss_fake: 4603.3506, D_loss_gp: 0.5461, G_loss: -4780.0161\n",
            "Step [2700/200000], D_loss_real: -5412.1802, D_loss_fake: 5351.5474, D_loss_gp: 1.2470, G_loss: -5713.2812\n",
            "Step [2800/200000], D_loss_real: -6272.0850, D_loss_fake: 6242.4707, D_loss_gp: 0.9623, G_loss: -5766.9717\n",
            "Step [2900/200000], D_loss_real: -4931.7939, D_loss_fake: 4887.3394, D_loss_gp: 0.5690, G_loss: -5548.6328\n",
            "Step [3000/200000], D_loss_real: -4827.2915, D_loss_fake: 4802.8672, D_loss_gp: 0.5917, G_loss: -4721.7139\n",
            "Step [3100/200000], D_loss_real: -5097.1553, D_loss_fake: 5056.6328, D_loss_gp: 0.7102, G_loss: -5386.1162\n",
            "Step [3200/200000], D_loss_real: -4632.5146, D_loss_fake: 4608.4111, D_loss_gp: 0.2904, G_loss: -4848.1851\n",
            "Step [3300/200000], D_loss_real: -5607.2656, D_loss_fake: 5569.3159, D_loss_gp: 1.6556, G_loss: -4768.9526\n",
            "Step [3400/200000], D_loss_real: -4443.3115, D_loss_fake: 4415.8145, D_loss_gp: 0.3548, G_loss: -4782.4722\n",
            "Step [3500/200000], D_loss_real: -4462.9902, D_loss_fake: 4412.5400, D_loss_gp: 0.4763, G_loss: -5443.4697\n",
            "Step [3600/200000], D_loss_real: -4312.4434, D_loss_fake: 4288.2363, D_loss_gp: 0.3907, G_loss: -4419.8169\n",
            "Step [3700/200000], D_loss_real: -4327.9385, D_loss_fake: 4295.1855, D_loss_gp: 0.5856, G_loss: -4660.9248\n",
            "Step [3800/200000], D_loss_real: -3451.6440, D_loss_fake: 3432.1348, D_loss_gp: 0.3745, G_loss: -3503.2568\n",
            "Step [3900/200000], D_loss_real: -3117.5942, D_loss_fake: 3092.0508, D_loss_gp: 0.8306, G_loss: -2983.2666\n",
            "Step [4000/200000], D_loss_real: -2415.2412, D_loss_fake: 2395.4932, D_loss_gp: 0.1721, G_loss: -2657.7073\n",
            "Step [4100/200000], D_loss_real: -2287.2812, D_loss_fake: 2266.9980, D_loss_gp: 0.3569, G_loss: -2336.8892\n",
            "Step [4200/200000], D_loss_real: -2791.7668, D_loss_fake: 2766.2297, D_loss_gp: 0.7026, G_loss: -2703.4502\n",
            "Step [4300/200000], D_loss_real: -2695.4009, D_loss_fake: 2673.7803, D_loss_gp: 0.3602, G_loss: -2736.0735\n",
            "Step [4400/200000], D_loss_real: -2939.8242, D_loss_fake: 2912.6948, D_loss_gp: 0.3914, G_loss: -3043.6353\n",
            "Step [4500/200000], D_loss_real: -2145.0381, D_loss_fake: 2129.8716, D_loss_gp: 0.1835, G_loss: -2207.2627\n",
            "Step [4600/200000], D_loss_real: -2618.1558, D_loss_fake: 2590.0945, D_loss_gp: 0.6351, G_loss: -2597.5222\n",
            "Step [4700/200000], D_loss_real: -2980.9409, D_loss_fake: 2956.6108, D_loss_gp: 0.6711, G_loss: -2828.9868\n",
            "Step [4800/200000], D_loss_real: -2420.9370, D_loss_fake: 2399.7095, D_loss_gp: 0.2059, G_loss: -2580.7686\n",
            "Step [4900/200000], D_loss_real: -2913.4167, D_loss_fake: 2892.5103, D_loss_gp: 0.5703, G_loss: -2757.6655\n",
            "Step [5000/200000], D_loss_real: -3158.8308, D_loss_fake: 3132.4463, D_loss_gp: 0.7017, G_loss: -3121.9714\n",
            "Step [5100/200000], D_loss_real: -3251.9463, D_loss_fake: 3226.2993, D_loss_gp: 0.4056, G_loss: -3415.8264\n",
            "Step [5200/200000], D_loss_real: -2663.8113, D_loss_fake: 2646.2988, D_loss_gp: 0.1672, G_loss: -2828.2976\n",
            "Step [5300/200000], D_loss_real: -3025.6445, D_loss_fake: 3007.5442, D_loss_gp: 0.3089, G_loss: -3137.5764\n",
            "Step [5400/200000], D_loss_real: -2834.8679, D_loss_fake: 2817.2505, D_loss_gp: 0.4567, G_loss: -2765.6951\n",
            "Step [5500/200000], D_loss_real: -3230.6147, D_loss_fake: 3211.8193, D_loss_gp: 0.4739, G_loss: -3067.7891\n",
            "Step [5600/200000], D_loss_real: -3105.9966, D_loss_fake: 3093.5635, D_loss_gp: 0.2884, G_loss: -2966.5723\n",
            "Step [5700/200000], D_loss_real: -2910.4631, D_loss_fake: 2893.6772, D_loss_gp: 0.3670, G_loss: -2865.8506\n",
            "Step [5800/200000], D_loss_real: -2794.1636, D_loss_fake: 2768.4282, D_loss_gp: 0.6844, G_loss: -2732.9973\n",
            "Step [5900/200000], D_loss_real: -2743.5435, D_loss_fake: 2705.9507, D_loss_gp: 0.3105, G_loss: -3027.2278\n",
            "Step [6000/200000], D_loss_real: -2502.9663, D_loss_fake: 2471.8889, D_loss_gp: 0.3192, G_loss: -2868.8721\n",
            "Step [6100/200000], D_loss_real: -2713.6111, D_loss_fake: 2698.3870, D_loss_gp: 0.5437, G_loss: -2579.8936\n",
            "Step [6200/200000], D_loss_real: -2669.7090, D_loss_fake: 2658.5310, D_loss_gp: 0.0465, G_loss: -2784.6543\n",
            "Step [6300/200000], D_loss_real: -2826.9390, D_loss_fake: 2784.3396, D_loss_gp: 0.9614, G_loss: -2842.2703\n",
            "Step [6400/200000], D_loss_real: -3125.3433, D_loss_fake: 3108.2881, D_loss_gp: 0.2344, G_loss: -3205.2656\n",
            "Step [6500/200000], D_loss_real: -2784.6416, D_loss_fake: 2755.0234, D_loss_gp: 0.2727, G_loss: -2959.8184\n",
            "Step [6600/200000], D_loss_real: -2932.8999, D_loss_fake: 2933.3506, D_loss_gp: 0.2750, G_loss: -2617.8599\n",
            "Step [6700/200000], D_loss_real: -2886.7188, D_loss_fake: 2870.3184, D_loss_gp: 0.1288, G_loss: -3039.9292\n",
            "Step [6800/200000], D_loss_real: -3153.7578, D_loss_fake: 3136.0103, D_loss_gp: 0.1821, G_loss: -3231.8372\n",
            "Step [6900/200000], D_loss_real: -3282.3076, D_loss_fake: 3265.4785, D_loss_gp: 0.3822, G_loss: -3222.9885\n",
            "Step [7000/200000], D_loss_real: -3038.3721, D_loss_fake: 3026.4458, D_loss_gp: 0.2499, G_loss: -2971.7642\n",
            "Step [7100/200000], D_loss_real: -2853.7900, D_loss_fake: 2841.7778, D_loss_gp: 0.1547, G_loss: -2875.0942\n",
            "Step [7200/200000], D_loss_real: -2750.7520, D_loss_fake: 2730.3796, D_loss_gp: 0.1961, G_loss: -2825.9214\n",
            "Step [7300/200000], D_loss_real: -3036.6514, D_loss_fake: 3024.5146, D_loss_gp: 0.2122, G_loss: -2986.1418\n",
            "Step [7400/200000], D_loss_real: -3172.9238, D_loss_fake: 3165.0957, D_loss_gp: 0.4888, G_loss: -2851.3774\n",
            "Step [7500/200000], D_loss_real: -3181.2654, D_loss_fake: 3172.5127, D_loss_gp: 0.2142, G_loss: -3056.5293\n",
            "Step [7600/200000], D_loss_real: -3195.5664, D_loss_fake: 3182.0962, D_loss_gp: 0.1131, G_loss: -3381.6367\n",
            "Step [7700/200000], D_loss_real: -3558.7969, D_loss_fake: 3541.5818, D_loss_gp: 0.2265, G_loss: -3644.8105\n",
            "Step [7800/200000], D_loss_real: -3197.4443, D_loss_fake: 3184.6174, D_loss_gp: 0.1538, G_loss: -3206.5249\n",
            "Step [7900/200000], D_loss_real: -3549.9409, D_loss_fake: 3545.9045, D_loss_gp: 0.4851, G_loss: -3190.7051\n",
            "Step [8000/200000], D_loss_real: -3077.6958, D_loss_fake: 3067.2544, D_loss_gp: 0.0764, G_loss: -3182.9255\n",
            "Step [8100/200000], D_loss_real: -3429.8921, D_loss_fake: 3419.6880, D_loss_gp: 0.2339, G_loss: -3327.9963\n",
            "Step [8200/200000], D_loss_real: -3114.5288, D_loss_fake: 3099.8667, D_loss_gp: 0.1481, G_loss: -3208.2861\n",
            "Step [8300/200000], D_loss_real: -3124.4390, D_loss_fake: 3114.0967, D_loss_gp: 0.0770, G_loss: -3134.1514\n",
            "Step [8400/200000], D_loss_real: -3249.8438, D_loss_fake: 3237.8667, D_loss_gp: 0.5051, G_loss: -2981.2837\n",
            "Step [8500/200000], D_loss_real: -2862.7490, D_loss_fake: 2841.6919, D_loss_gp: 0.4174, G_loss: -2855.4297\n",
            "Step [8600/200000], D_loss_real: -3128.7104, D_loss_fake: 3114.9653, D_loss_gp: 0.1985, G_loss: -3205.6990\n",
            "Step [8700/200000], D_loss_real: -2583.8301, D_loss_fake: 2574.6016, D_loss_gp: 0.0173, G_loss: -2655.4702\n",
            "Step [8800/200000], D_loss_real: -3119.4453, D_loss_fake: 3108.3174, D_loss_gp: 0.0907, G_loss: -3165.1060\n",
            "Step [8900/200000], D_loss_real: -3283.2710, D_loss_fake: 3265.9165, D_loss_gp: 0.1696, G_loss: -3435.8911\n",
            "Step [9000/200000], D_loss_real: -2966.7588, D_loss_fake: 2950.7876, D_loss_gp: 0.1111, G_loss: -3040.9834\n",
            "Step [9100/200000], D_loss_real: -2713.2173, D_loss_fake: 2706.7124, D_loss_gp: 0.1571, G_loss: -2624.3716\n",
            "Step [9200/200000], D_loss_real: -2707.9978, D_loss_fake: 2696.7800, D_loss_gp: 0.2693, G_loss: -2701.5903\n",
            "Step [9300/200000], D_loss_real: -2950.2112, D_loss_fake: 2935.9854, D_loss_gp: 0.2596, G_loss: -2940.4604\n",
            "Step [9400/200000], D_loss_real: -3337.4495, D_loss_fake: 3336.6768, D_loss_gp: 0.1674, G_loss: -3200.8770\n",
            "Step [9500/200000], D_loss_real: -3338.6104, D_loss_fake: 3333.5801, D_loss_gp: 0.0585, G_loss: -3357.5034\n",
            "Step [9600/200000], D_loss_real: -3935.8335, D_loss_fake: 3925.5317, D_loss_gp: 0.3891, G_loss: -3763.7202\n",
            "Step [9700/200000], D_loss_real: -3265.1772, D_loss_fake: 3258.4233, D_loss_gp: 0.0229, G_loss: -3327.2754\n",
            "Step [9800/200000], D_loss_real: -3145.9414, D_loss_fake: 3136.9468, D_loss_gp: 0.0476, G_loss: -3200.4700\n",
            "Step [9900/200000], D_loss_real: -3452.7622, D_loss_fake: 3445.4976, D_loss_gp: 0.4244, G_loss: -3212.1892\n",
            "Step [10000/200000], D_loss_real: -3712.9504, D_loss_fake: 3697.0999, D_loss_gp: 0.0681, G_loss: -3895.4131\n",
            "Step [10100/200000], D_loss_real: -3462.5889, D_loss_fake: 3449.5522, D_loss_gp: 0.0557, G_loss: -3591.0752\n",
            "Step [10200/200000], D_loss_real: -3311.3853, D_loss_fake: 3301.1775, D_loss_gp: 0.0822, G_loss: -3398.7605\n",
            "Step [10300/200000], D_loss_real: -3591.8867, D_loss_fake: 3576.8518, D_loss_gp: 0.1165, G_loss: -3694.0649\n",
            "Step [10400/200000], D_loss_real: -3571.6523, D_loss_fake: 3561.3643, D_loss_gp: 0.1433, G_loss: -3552.4360\n",
            "Step [10500/200000], D_loss_real: -4313.4248, D_loss_fake: 4304.3477, D_loss_gp: 0.2032, G_loss: -4163.9570\n",
            "Step [10600/200000], D_loss_real: -4111.5332, D_loss_fake: 4106.0557, D_loss_gp: 0.0978, G_loss: -4029.8916\n",
            "Step [10700/200000], D_loss_real: -3905.4841, D_loss_fake: 3894.5083, D_loss_gp: 0.0888, G_loss: -3972.4517\n",
            "Step [10800/200000], D_loss_real: -3606.7524, D_loss_fake: 3602.0171, D_loss_gp: 0.0170, G_loss: -3720.1621\n",
            "Step [10900/200000], D_loss_real: -4135.2661, D_loss_fake: 4125.1152, D_loss_gp: 0.1252, G_loss: -4112.3750\n",
            "Step [11000/200000], D_loss_real: -4212.7876, D_loss_fake: 4198.7441, D_loss_gp: 0.0722, G_loss: -4402.4038\n",
            "Step [11100/200000], D_loss_real: -4270.1470, D_loss_fake: 4258.7456, D_loss_gp: 0.2133, G_loss: -4210.9155\n",
            "Step [11200/200000], D_loss_real: -4010.4553, D_loss_fake: 4002.1704, D_loss_gp: 0.0549, G_loss: -4097.8740\n",
            "Step [11300/200000], D_loss_real: -4237.7012, D_loss_fake: 4228.6577, D_loss_gp: 0.1258, G_loss: -4206.0059\n",
            "Step [11400/200000], D_loss_real: -4061.7358, D_loss_fake: 4049.1914, D_loss_gp: 0.2151, G_loss: -4035.0442\n",
            "Step [11500/200000], D_loss_real: -3965.8787, D_loss_fake: 3961.5095, D_loss_gp: 0.0741, G_loss: -3850.7656\n",
            "Step [11600/200000], D_loss_real: -4441.3765, D_loss_fake: 4430.7090, D_loss_gp: 0.1207, G_loss: -4527.3232\n",
            "Step [11700/200000], D_loss_real: -4969.2808, D_loss_fake: 4958.4932, D_loss_gp: 0.0604, G_loss: -5058.9951\n",
            "Step [11800/200000], D_loss_real: -4773.3857, D_loss_fake: 4770.0977, D_loss_gp: 0.1990, G_loss: -4577.1318\n",
            "Step [11900/200000], D_loss_real: -5563.8921, D_loss_fake: 5548.8779, D_loss_gp: 0.1712, G_loss: -5590.7422\n",
            "Step [12000/200000], D_loss_real: -4993.0771, D_loss_fake: 4987.3848, D_loss_gp: 0.0341, G_loss: -5017.1206\n",
            "Step [12100/200000], D_loss_real: -4852.4399, D_loss_fake: 4845.3799, D_loss_gp: 0.0484, G_loss: -4944.8408\n",
            "Step [12200/200000], D_loss_real: -4959.6812, D_loss_fake: 4949.3940, D_loss_gp: 0.0483, G_loss: -5121.3608\n",
            "Step [12300/200000], D_loss_real: -5111.0732, D_loss_fake: 5102.9546, D_loss_gp: 0.0450, G_loss: -5275.7012\n",
            "Step [12400/200000], D_loss_real: -5030.2437, D_loss_fake: 5018.6895, D_loss_gp: 0.3608, G_loss: -4925.6943\n",
            "Step [12500/200000], D_loss_real: -5027.7734, D_loss_fake: 5023.7910, D_loss_gp: 0.1431, G_loss: -4946.2979\n",
            "Step [12600/200000], D_loss_real: -5560.3008, D_loss_fake: 5551.8877, D_loss_gp: 0.1155, G_loss: -5570.4365\n",
            "Step [12700/200000], D_loss_real: -5554.3457, D_loss_fake: 5544.0913, D_loss_gp: 0.1952, G_loss: -5585.5396\n",
            "Step [12800/200000], D_loss_real: -6092.4160, D_loss_fake: 6089.1675, D_loss_gp: 0.5729, G_loss: -5688.5767\n",
            "Step [12900/200000], D_loss_real: -6219.0361, D_loss_fake: 6217.2861, D_loss_gp: 0.1245, G_loss: -6088.3828\n",
            "Step [13000/200000], D_loss_real: -6099.7920, D_loss_fake: 6089.2939, D_loss_gp: 0.0425, G_loss: -6205.5400\n",
            "Step [13100/200000], D_loss_real: -7009.9365, D_loss_fake: 6989.4072, D_loss_gp: 0.4112, G_loss: -6922.7085\n",
            "Step [13200/200000], D_loss_real: -6597.6602, D_loss_fake: 6591.0312, D_loss_gp: 0.0557, G_loss: -6623.7568\n",
            "Step [13300/200000], D_loss_real: -7344.0967, D_loss_fake: 7326.0376, D_loss_gp: 0.3095, G_loss: -7436.2261\n",
            "Step [13400/200000], D_loss_real: -6960.6914, D_loss_fake: 6950.1812, D_loss_gp: 0.1043, G_loss: -6961.6250\n",
            "Step [13500/200000], D_loss_real: -6841.9634, D_loss_fake: 6824.9624, D_loss_gp: 0.2522, G_loss: -6879.2847\n",
            "Step [13600/200000], D_loss_real: -6719.8848, D_loss_fake: 6716.1943, D_loss_gp: 0.0339, G_loss: -6789.8848\n",
            "Step [13700/200000], D_loss_real: -7463.9678, D_loss_fake: 7459.3667, D_loss_gp: 0.0433, G_loss: -7492.4756\n",
            "Step [13800/200000], D_loss_real: -7470.1392, D_loss_fake: 7461.8560, D_loss_gp: 0.0937, G_loss: -7502.6289\n",
            "Step [13900/200000], D_loss_real: -7197.0879, D_loss_fake: 7187.3916, D_loss_gp: 0.0665, G_loss: -7259.6289\n",
            "Step [14000/200000], D_loss_real: -6923.8687, D_loss_fake: 6912.8135, D_loss_gp: 0.1266, G_loss: -6973.9497\n",
            "Step [14100/200000], D_loss_real: -7061.6392, D_loss_fake: 7046.5752, D_loss_gp: 0.1414, G_loss: -7182.9375\n",
            "Step [14200/200000], D_loss_real: -7628.9941, D_loss_fake: 7616.8037, D_loss_gp: 0.0909, G_loss: -7793.7710\n",
            "Step [14300/200000], D_loss_real: -8259.3994, D_loss_fake: 8236.9453, D_loss_gp: 0.1046, G_loss: -8639.6465\n",
            "Step [14400/200000], D_loss_real: -7972.2500, D_loss_fake: 7965.4258, D_loss_gp: 0.0644, G_loss: -8008.3779\n",
            "Step [14500/200000], D_loss_real: -8133.3179, D_loss_fake: 8128.1348, D_loss_gp: 0.0643, G_loss: -8120.6895\n",
            "Step [14600/200000], D_loss_real: -8000.9023, D_loss_fake: 7991.9600, D_loss_gp: 0.0592, G_loss: -8129.2192\n",
            "Step [14700/200000], D_loss_real: -8749.1953, D_loss_fake: 8735.3164, D_loss_gp: 0.2693, G_loss: -8690.5879\n",
            "Step [14800/200000], D_loss_real: -8601.5342, D_loss_fake: 8595.1582, D_loss_gp: 0.1564, G_loss: -8471.7500\n",
            "Step [14900/200000], D_loss_real: -8439.9199, D_loss_fake: 8426.5049, D_loss_gp: 0.0820, G_loss: -8674.2959\n",
            "Step [15000/200000], D_loss_real: -8100.0391, D_loss_fake: 8080.4512, D_loss_gp: 0.1247, G_loss: -8378.0078\n",
            "Step [15100/200000], D_loss_real: -8350.2773, D_loss_fake: 8338.1367, D_loss_gp: 0.1755, G_loss: -8337.7090\n",
            "Step [15200/200000], D_loss_real: -9047.8613, D_loss_fake: 9032.7109, D_loss_gp: 0.2429, G_loss: -9134.6328\n",
            "Step [15300/200000], D_loss_real: -9311.7188, D_loss_fake: 9287.7461, D_loss_gp: 0.4295, G_loss: -9364.8711\n",
            "Step [15400/200000], D_loss_real: -8749.8633, D_loss_fake: 8741.7715, D_loss_gp: 0.0815, G_loss: -8808.8047\n",
            "Step [15500/200000], D_loss_real: -9356.7959, D_loss_fake: 9348.6182, D_loss_gp: 0.0254, G_loss: -9521.3516\n",
            "Step [15600/200000], D_loss_real: -9248.2334, D_loss_fake: 9239.4326, D_loss_gp: 0.3713, G_loss: -8910.7930\n",
            "Step [15700/200000], D_loss_real: -8630.7227, D_loss_fake: 8627.8730, D_loss_gp: 0.0470, G_loss: -8594.2988\n",
            "Step [15800/200000], D_loss_real: -10072.0176, D_loss_fake: 10053.7207, D_loss_gp: 0.4348, G_loss: -9945.9355\n",
            "Step [15900/200000], D_loss_real: -9599.7197, D_loss_fake: 9590.1445, D_loss_gp: 0.0429, G_loss: -9917.2031\n",
            "Step [16000/200000], D_loss_real: -9630.0469, D_loss_fake: 9624.8633, D_loss_gp: 0.0223, G_loss: -9769.0840\n",
            "Step [16100/200000], D_loss_real: -10487.2148, D_loss_fake: 10475.5352, D_loss_gp: 0.1048, G_loss: -10607.0908\n",
            "Step [16200/200000], D_loss_real: -10829.8184, D_loss_fake: 10820.7930, D_loss_gp: 0.3120, G_loss: -10532.7705\n",
            "Step [16300/200000], D_loss_real: -10779.5703, D_loss_fake: 10765.1133, D_loss_gp: 0.1286, G_loss: -10991.3457\n",
            "Step [16400/200000], D_loss_real: -11217.7256, D_loss_fake: 11208.4258, D_loss_gp: 0.0322, G_loss: -11395.4580\n",
            "Step [16500/200000], D_loss_real: -11075.0957, D_loss_fake: 11078.4932, D_loss_gp: 0.2090, G_loss: -10612.0430\n",
            "Step [16600/200000], D_loss_real: -10626.7100, D_loss_fake: 10617.9863, D_loss_gp: 0.1308, G_loss: -10626.6338\n",
            "Step [16700/200000], D_loss_real: -10692.3311, D_loss_fake: 10680.3652, D_loss_gp: 0.3395, G_loss: -10410.8848\n",
            "Step [16800/200000], D_loss_real: -10521.0576, D_loss_fake: 10505.8574, D_loss_gp: 0.0996, G_loss: -10777.6914\n",
            "Step [16900/200000], D_loss_real: -10881.2012, D_loss_fake: 10874.4258, D_loss_gp: 0.0967, G_loss: -10867.6152\n",
            "Step [17000/200000], D_loss_real: -10086.6895, D_loss_fake: 10072.7002, D_loss_gp: 0.1109, G_loss: -10273.2891\n",
            "Step [17100/200000], D_loss_real: -11225.1328, D_loss_fake: 11213.7715, D_loss_gp: 0.0683, G_loss: -11483.5342\n",
            "Step [17200/200000], D_loss_real: -12014.9512, D_loss_fake: 11998.5830, D_loss_gp: 0.1039, G_loss: -12317.9082\n",
            "Step [17300/200000], D_loss_real: -10798.8135, D_loss_fake: 10785.7930, D_loss_gp: 0.4117, G_loss: -10718.5205\n",
            "Step [17400/200000], D_loss_real: -10077.3438, D_loss_fake: 10066.5020, D_loss_gp: 0.0445, G_loss: -10233.3906\n",
            "Step [17500/200000], D_loss_real: -9512.0928, D_loss_fake: 9503.5225, D_loss_gp: 0.0762, G_loss: -9602.3359\n",
            "Step [17600/200000], D_loss_real: -9464.9746, D_loss_fake: 9452.3027, D_loss_gp: 0.0630, G_loss: -9705.6016\n",
            "Step [17700/200000], D_loss_real: -9527.2959, D_loss_fake: 9513.1387, D_loss_gp: 0.0870, G_loss: -9852.4590\n",
            "Step [17800/200000], D_loss_real: -10336.6934, D_loss_fake: 10327.3418, D_loss_gp: 0.0809, G_loss: -10411.7861\n",
            "Step [17900/200000], D_loss_real: -10502.2227, D_loss_fake: 10486.4893, D_loss_gp: 0.1535, G_loss: -10640.4785\n",
            "Step [18000/200000], D_loss_real: -10850.7988, D_loss_fake: 10845.5840, D_loss_gp: 0.2585, G_loss: -10467.6592\n",
            "Step [18100/200000], D_loss_real: -10105.3125, D_loss_fake: 10096.2500, D_loss_gp: 0.0492, G_loss: -10294.7051\n",
            "Step [18200/200000], D_loss_real: -11290.8779, D_loss_fake: 11279.2529, D_loss_gp: 0.0687, G_loss: -11454.2842\n",
            "Step [18300/200000], D_loss_real: -11611.6465, D_loss_fake: 11597.6104, D_loss_gp: 0.1715, G_loss: -11704.0664\n",
            "Step [18400/200000], D_loss_real: -11897.4805, D_loss_fake: 11890.5283, D_loss_gp: 0.1370, G_loss: -11821.3623\n",
            "Step [18500/200000], D_loss_real: -11871.2734, D_loss_fake: 11868.4502, D_loss_gp: 0.0231, G_loss: -11860.9893\n",
            "Step [18600/200000], D_loss_real: -11766.7207, D_loss_fake: 11762.5117, D_loss_gp: 0.0328, G_loss: -11790.7148\n",
            "Step [18700/200000], D_loss_real: -11439.3613, D_loss_fake: 11435.9395, D_loss_gp: 0.0310, G_loss: -11474.0000\n",
            "Step [18800/200000], D_loss_real: -11549.3955, D_loss_fake: 11542.3857, D_loss_gp: 0.0512, G_loss: -11654.8770\n",
            "Step [18900/200000], D_loss_real: -11014.5977, D_loss_fake: 11016.8789, D_loss_gp: 0.0292, G_loss: -10803.5801\n",
            "Step [19000/200000], D_loss_real: -11069.5000, D_loss_fake: 11059.3926, D_loss_gp: 0.0438, G_loss: -11399.0527\n",
            "Step [19100/200000], D_loss_real: -11382.0566, D_loss_fake: 11373.3496, D_loss_gp: 0.0564, G_loss: -11490.1953\n",
            "Step [19200/200000], D_loss_real: -10872.0586, D_loss_fake: 10864.4092, D_loss_gp: 0.1079, G_loss: -10853.1895\n",
            "Step [19300/200000], D_loss_real: -12325.2578, D_loss_fake: 12315.4160, D_loss_gp: 0.3362, G_loss: -12046.1133\n",
            "Step [19400/200000], D_loss_real: -10893.6738, D_loss_fake: 10883.0449, D_loss_gp: 0.1293, G_loss: -10880.2891\n",
            "Step [19500/200000], D_loss_real: -11042.7422, D_loss_fake: 11033.8066, D_loss_gp: 0.0378, G_loss: -11200.2520\n",
            "Step [19600/200000], D_loss_real: -11720.2930, D_loss_fake: 11710.0625, D_loss_gp: 0.0709, G_loss: -11872.7070\n",
            "Step [19700/200000], D_loss_real: -12031.2363, D_loss_fake: 12020.4502, D_loss_gp: 0.0879, G_loss: -12189.9893\n",
            "Step [19800/200000], D_loss_real: -13012.7676, D_loss_fake: 13004.7490, D_loss_gp: 0.1717, G_loss: -12860.2656\n",
            "Step [19900/200000], D_loss_real: -11366.8496, D_loss_fake: 11358.1885, D_loss_gp: 0.0698, G_loss: -11435.0957\n",
            "Step [20000/200000], D_loss_real: -11453.1797, D_loss_fake: 11447.8330, D_loss_gp: 0.0159, G_loss: -11596.4414\n",
            "Step [20100/200000], D_loss_real: -11320.2441, D_loss_fake: 11311.0723, D_loss_gp: 0.1102, G_loss: -11356.9756\n",
            "Step [20200/200000], D_loss_real: -12124.2363, D_loss_fake: 12116.4082, D_loss_gp: 0.1337, G_loss: -12092.1104\n",
            "Step [20300/200000], D_loss_real: -11631.7559, D_loss_fake: 11626.2266, D_loss_gp: 0.1588, G_loss: -11544.7236\n",
            "Step [20400/200000], D_loss_real: -12408.0352, D_loss_fake: 12397.5664, D_loss_gp: 0.0750, G_loss: -12696.8418\n",
            "Step [20500/200000], D_loss_real: -11462.6055, D_loss_fake: 11456.5762, D_loss_gp: 0.0489, G_loss: -11639.0156\n",
            "Step [20600/200000], D_loss_real: -11726.5557, D_loss_fake: 11716.1719, D_loss_gp: 0.0192, G_loss: -11971.5352\n",
            "Step [20700/200000], D_loss_real: -12151.0645, D_loss_fake: 12143.4307, D_loss_gp: 0.0657, G_loss: -12228.4736\n",
            "Step [20800/200000], D_loss_real: -12517.3525, D_loss_fake: 12503.6836, D_loss_gp: 0.0445, G_loss: -12865.4678\n",
            "Step [20900/200000], D_loss_real: -12204.5859, D_loss_fake: 12192.1758, D_loss_gp: 0.1537, G_loss: -12414.6143\n",
            "Step [21000/200000], D_loss_real: -13293.9160, D_loss_fake: 13283.7324, D_loss_gp: 0.0708, G_loss: -13401.0967\n",
            "Step [21100/200000], D_loss_real: -12926.8555, D_loss_fake: 12920.5293, D_loss_gp: 0.0803, G_loss: -12941.7979\n",
            "Step [21200/200000], D_loss_real: -13071.1973, D_loss_fake: 13062.6387, D_loss_gp: 0.0131, G_loss: -13258.9248\n",
            "Step [21300/200000], D_loss_real: -13967.6787, D_loss_fake: 13954.8828, D_loss_gp: 0.0590, G_loss: -14262.1113\n",
            "Step [21400/200000], D_loss_real: -13964.1367, D_loss_fake: 13955.7949, D_loss_gp: 0.1210, G_loss: -13899.4980\n",
            "Step [21500/200000], D_loss_real: -13514.7090, D_loss_fake: 13506.2705, D_loss_gp: 0.3111, G_loss: -13342.2354\n",
            "Step [21600/200000], D_loss_real: -13931.8594, D_loss_fake: 13916.3379, D_loss_gp: 0.0282, G_loss: -14310.9990\n",
            "Step [21700/200000], D_loss_real: -13804.4912, D_loss_fake: 13794.1816, D_loss_gp: 0.0823, G_loss: -14021.2324\n",
            "Step [21800/200000], D_loss_real: -13895.7959, D_loss_fake: 13893.3662, D_loss_gp: 0.0481, G_loss: -13802.0898\n",
            "Step [21900/200000], D_loss_real: -13467.6875, D_loss_fake: 13462.5254, D_loss_gp: 0.0282, G_loss: -13571.3486\n",
            "Step [22000/200000], D_loss_real: -13594.0439, D_loss_fake: 13588.4482, D_loss_gp: 0.0358, G_loss: -13643.7178\n",
            "Step [22100/200000], D_loss_real: -14851.7949, D_loss_fake: 14849.7920, D_loss_gp: 0.0652, G_loss: -14578.4580\n",
            "Step [22200/200000], D_loss_real: -13796.0195, D_loss_fake: 13797.4492, D_loss_gp: 0.0401, G_loss: -13587.2617\n",
            "Step [22300/200000], D_loss_real: -13358.6582, D_loss_fake: 13346.8301, D_loss_gp: 0.0349, G_loss: -13774.6309\n",
            "Step [22400/200000], D_loss_real: -14521.3135, D_loss_fake: 14510.6855, D_loss_gp: 0.1220, G_loss: -14588.5830\n",
            "Step [22500/200000], D_loss_real: -14065.0684, D_loss_fake: 14056.7617, D_loss_gp: 0.0190, G_loss: -14333.7988\n",
            "Step [22600/200000], D_loss_real: -14506.5264, D_loss_fake: 14499.1689, D_loss_gp: 0.0683, G_loss: -14648.6660\n",
            "Step [22700/200000], D_loss_real: -15114.2842, D_loss_fake: 15109.6387, D_loss_gp: 0.0252, G_loss: -15241.0010\n",
            "Step [22800/200000], D_loss_real: -15826.6660, D_loss_fake: 15825.8691, D_loss_gp: 0.0670, G_loss: -15454.7080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Plot Generator and Discriminator losses**"
      ],
      "metadata": {
        "id": "8OwIy5pNMkWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SAMPLE_OUTPUT_DIR = os.path.join('/content/samples', 'Gan_flower102')\n",
        "\n",
        "# --- Plot 1: Discriminator Total Loss ---\n",
        "plot_path_d = os.path.join(SAMPLE_OUTPUT_DIR, 'discriminator_total_loss.png')\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(steps_history, d_total_loss_history, label='Discriminator Total Loss', color='blue', alpha=0.8)\n",
        "plt.xlabel('Training Step')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Discriminator Total Loss Over Time')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(plot_path_d)\n",
        "plt.show()\n",
        "print(f\"Discriminator loss plot saved to: {plot_path_d}\")\n",
        "\n",
        "# --- Plot 2: Generator Loss ---\n",
        "plot_path_g = os.path.join(SAMPLE_OUTPUT_DIR, 'generator_loss.png')\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(steps_history, g_total_loss_history, label='Generator Loss', color='red', alpha=0.8)\n",
        "plt.xlabel('Training Step')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Generator Loss Over Time')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(plot_path_g)\n",
        "plt.show()\n",
        "print(f\"Generator loss plot saved to: {plot_path_g}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "22dj35o4MiWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Animation of Training Progress**"
      ],
      "metadata": {
        "id": "C4PtAvnxsxAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_animation(sample_dir, output_gif_name=\"training_progress.gif\", fps=10):\n",
        "    image_files = []\n",
        "    sorted_files = sorted(\n",
        "        [f for f in os.listdir(sample_dir) if f.endswith('_generated.png')],\n",
        "        key=lambda x: int(re.findall(r'(\\d+)_generated\\.png', x)[0]) if re.findall(r'(\\d+)_generated\\.png', x) else 0\n",
        "    )\n",
        "\n",
        "    for f in sorted_files:\n",
        "        image_files.append(os.path.join(sample_dir, f))\n",
        "\n",
        "    images_for_gif = []\n",
        "    for image_file in image_files:\n",
        "      img = Image.open(image_file).convert('RGB')\n",
        "      images_for_gif.append(np.array(img))\n",
        "\n",
        "    fps = 10\n",
        "\n",
        "    output_path = os.path.join(sample_dir, output_gif_name)\n",
        "\n",
        "    imageio.mimsave(output_path, images_for_gif, fps=fps)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(images_for_gif[0])\n",
        "    ax.axis('off')\n",
        "    plt.title('GAN Training Progress Animation')\n",
        "\n",
        "    def update(frame):\n",
        "        im.set_array(images_for_gif[frame])\n",
        "        return [im]\n",
        "\n",
        "    ani = animation.FuncAnimation(\n",
        "        fig, update, frames=len(images_for_gif),\n",
        "        interval=1000 / fps, blit=True, repeat=False\n",
        "    )\n",
        "\n",
        "    plt.show(block=False)\n",
        "\n",
        "create_animation(SAMPLE_OUTPUT_DIR)\n"
      ],
      "metadata": {
        "id": "4jy6V9Xl3VCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Plot Real Images vs Fake Images**"
      ],
      "metadata": {
        "id": "BRDgk0THs9V8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_real_vs_fake(real_image, generated_image):\n",
        "    real_img = Image.open(real_image)\n",
        "    final_fake_img = Image.open(generated_image)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "    axes[0].imshow(real_img)\n",
        "    axes[0].set_title('Sample Real Images')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    axes[1].imshow(final_fake_img)\n",
        "    axes[1].set_title('Final Generated Images')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    plt.suptitle('Real vs. Generated Images After Training', fontsize=16)\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    output_comparison_path = os.path.join(SAMPLE_OUTPUT_DIR, 'real_vs_fake_comparison.png')\n",
        "    plt.savefig(output_comparison_path)\n",
        "    plt.show()\n",
        "\n",
        "final_step_str = str(100000)\n",
        "generated_image_sample = os.path.join(SAMPLE_OUTPUT_DIR, f'{final_step_str}_generated.png')\n",
        "real_image_sample = os.path.join(SAMPLE_OUTPUT_DIR, 'real_images_sample.png')\n",
        "\n",
        "display_real_vs_fake(real_image=real_image_sample, generated_image=generated_image_sample)\n"
      ],
      "metadata": {
        "id": "YpBUWaFDVL9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Ev9D4LJjypL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}