{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **BigGAN Implmentation for Oxford 102 Flowers**"
      ],
      "metadata": {
        "id": "ZYxGktCqacte"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Imports**"
      ],
      "metadata": {
        "id": "XLjrBVCoaUFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-fid"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCeX8bEbfFi5",
        "outputId": "41e7c780-5623-48d2-8944-342c2c017ea2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-fid\n",
            "  Downloading pytorch_fid-0.3.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pytorch-fid) (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from pytorch-fid) (11.2.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pytorch-fid) (1.15.3)\n",
            "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from pytorch-fid) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from pytorch-fid) (0.21.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.0.1->pytorch-fid)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.0.1->pytorch-fid)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.0.1->pytorch-fid)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.0.1->pytorch-fid)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.0.1->pytorch-fid)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.0.1->pytorch-fid)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.0.1->pytorch-fid)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.0.1->pytorch-fid)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.0.1->pytorch-fid)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.0.1->pytorch-fid)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.1->pytorch-fid) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.1->pytorch-fid) (3.0.2)\n",
            "Downloading pytorch_fid-0.3.0-py3-none-any.whl (15 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m128.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m107.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, pytorch-fid\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pytorch-fid-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import tarfile\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import argparse\n",
        "import types\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn import functional as F\n",
        "from torch.backends import cudnn\n",
        "from torchvision.utils import save_image\n",
        "import torchvision.datasets as dsets\n",
        "from torchvision import transforms\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from PIL import Image\n",
        "import imageio.v2 as imageio\n",
        "import re\n",
        "import matplotlib.animation as animation\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.transforms import ToPILImage\n",
        "import shutil\n",
        "from tqdm.auto import tqdm\n",
        "import pytorch_fid"
      ],
      "metadata": {
        "id": "JxdB0A_-XZ3R"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Download Data**"
      ],
      "metadata": {
        "id": "RXX-1LgkTOQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"/content/flowers_data\", exist_ok=True)\n",
        "url = \"https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz\"\n",
        "local_path = \"/content/flowers_data/102flowers.tgz\"\n",
        "\n",
        "urllib.request.urlretrieve(url, local_path)\n",
        "\n",
        "with tarfile.open(local_path) as tar:\n",
        "    tar.extractall(path=\"/content/flowers\")\n"
      ],
      "metadata": {
        "id": "ozbIVVnhTMmn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Helper functions**"
      ],
      "metadata": {
        "id": "JaRviC4Bakff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_output_directory(base_path: str, sub_version: str = \"\") -> str:\n",
        "    full_path = os.path.join(base_path, sub_version) if sub_version else base_path\n",
        "    os.makedirs(full_path, exist_ok=True)\n",
        "    return full_path\n",
        "\n",
        "def move_tensor_to_device(data: torch.Tensor, device: torch.device) -> torch.Tensor:\n",
        "    return data.to(device)\n",
        "\n",
        "def denormalize_image(x: torch.Tensor) -> torch.Tensor:\n",
        "    return ((x + 1) / 2).clamp_(0, 1)\n",
        "\n",
        "def initialize_network_weights(module):\n",
        "    classname = module.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        init.xavier_normal_(module.weight.data)\n",
        "        if module.bias is not None:\n",
        "            init.constant_(module.bias.data, 0.0)\n",
        "    elif classname.find('Linear') != -1:\n",
        "        init.xavier_normal_(module.weight.data)\n",
        "        if module.bias is not None:\n",
        "            init.constant_(module.bias.data, 0.0)\n",
        "\n",
        "def l2_normalize_vector(v, epsilon=1e-12):\n",
        "    return v / (v.norm() + epsilon)\n"
      ],
      "metadata": {
        "id": "YtVOTZ9aZyjF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Normalization**"
      ],
      "metadata": {
        "id": "sN1h8wUEarPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SpectralNormalization(nn.Module):\n",
        "    def __init__(self, module, name='weight', power_iterations=1):\n",
        "        super(SpectralNormalization, self).__init__()\n",
        "        self.module = module\n",
        "        self.name = name\n",
        "        self.power_iterations = power_iterations\n",
        "        if not hasattr(self.module, self.name + \"_u\"):\n",
        "            self._add_spectral_params()\n",
        "\n",
        "    def _update_uv_vectors(self):\n",
        "        u = getattr(self.module, self.name + \"_u\")\n",
        "        v = getattr(self.module, self.name + \"_v\")\n",
        "        w_bar = getattr(self.module, self.name + \"_bar\")\n",
        "\n",
        "        height = w_bar.data.shape[0]\n",
        "        w_matrix = w_bar.view(height, -1).data\n",
        "\n",
        "        for _ in range(self.power_iterations):\n",
        "            v.data = l2_normalize_vector(torch.mv(w_matrix.T, u.data))\n",
        "            u.data = l2_normalize_vector(torch.mv(w_matrix, v.data))\n",
        "\n",
        "        sigma = u.dot(w_matrix.mv(v))\n",
        "\n",
        "        setattr(self.module, self.name, w_bar / sigma.expand_as(w_bar))\n",
        "\n",
        "    def _add_spectral_params(self):\n",
        "        original_weight = getattr(self.module, self.name)\n",
        "\n",
        "        height = original_weight.data.shape[0]\n",
        "        width = original_weight.view(height, -1).data.shape[1]\n",
        "\n",
        "        u = Parameter(original_weight.data.new(height).normal_(0, 1), requires_grad=False)\n",
        "        v = Parameter(original_weight.data.new(width).normal_(0, 1), requires_grad=False)\n",
        "        u.data = l2_normalize_vector(u.data)\n",
        "        v.data = l2_normalize_vector(v.data)\n",
        "\n",
        "        w_bar = Parameter(original_weight.data)\n",
        "        del self.module._parameters[self.name]\n",
        "\n",
        "        self.module.register_parameter(self.name + \"_u\", u)\n",
        "        self.module.register_parameter(self.name + \"_v\", v)\n",
        "        self.module.register_parameter(self.name + \"_bar\", w_bar)\n",
        "\n",
        "    def forward(self, *args):\n",
        "        self._update_uv_vectors()\n",
        "        return self.module.forward(*args)\n",
        "\n",
        "class AdaptiveNormalization(nn.Module):\n",
        "    def __init__(self, channels, condition_dim=148):\n",
        "        super().__init__()\n",
        "        self.batch_norm = nn.BatchNorm2d(channels, affine=False)\n",
        "        self.embedding_layer = nn.Linear(condition_dim, channels * 2)\n",
        "\n",
        "        nn.init.constant_(self.embedding_layer.weight.data[:, :channels], 1.0)\n",
        "        nn.init.constant_(self.embedding_layer.weight.data[:, channels:], 0.0)\n",
        "        if self.embedding_layer.bias is not None:\n",
        "            nn.init.constant_(self.embedding_layer.bias.data[:channels], 1.0) # Gamma bias\n",
        "            nn.init.constant_(self.embedding_layer.bias.data[channels:], 0.0) # Beta bias\n",
        "\n",
        "    def forward(self, input_tensor, condition_vector):\n",
        "        normalized_output = self.batch_norm(input_tensor)\n",
        "\n",
        "        gamma_beta = self.embedding_layer(condition_vector)\n",
        "        gamma, beta = gamma_beta.chunk(2, 1)\n",
        "\n",
        "        # Reshape gamma and beta for broadcasting\n",
        "        gamma = gamma.unsqueeze(2).unsqueeze(3)\n",
        "        beta = beta.unsqueeze(2).unsqueeze(3)\n",
        "\n",
        "        output = gamma * normalized_output + beta\n",
        "        return output\n",
        "\n"
      ],
      "metadata": {
        "id": "vJMuYabzZvVW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Attention Module**"
      ],
      "metadata": {
        "id": "9MIiowX4bOgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SpatialAttentionBlock(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(SpatialAttentionBlock,self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        self.query_conv = SpectralNormalization(nn.Conv2d(in_channels, in_channels // 8, kernel_size=1))\n",
        "        self.key_conv = SpectralNormalization(nn.Conv2d(in_channels, in_channels // 8, kernel_size=1))\n",
        "        self.value_conv = SpectralNormalization(nn.Conv2d(in_channels, in_channels, kernel_size=1))\n",
        "\n",
        "        # Learnable gamma parameter for weighted sum\n",
        "        self.gamma_param = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "        self.softmax_fn = nn.Softmax(dim=-1)\n",
        "        self.post_attention_conv = SpectralNormalization(nn.Conv2d(in_channels, in_channels, kernel_size=1))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, C, H, W = x.size()\n",
        "\n",
        "        # Reshape for matrix multiplication\n",
        "        proj_query = self.query_conv(x).view(batch_size, -1, H * W).permute(0, 2, 1) # B x HW x C'\n",
        "        proj_key = self.key_conv(x).view(batch_size, -1, H * W)                     # B x C' x HW\n",
        "        proj_value = self.value_conv(x).view(batch_size, -1, H * W)                 # B x C x HW\n",
        "\n",
        "        # Calculate attention map\n",
        "        energy = torch.bmm(proj_query, proj_key) # B x HW x HW\n",
        "        attention = self.softmax_fn(energy)      # B x HW x HW\n",
        "\n",
        "        # Apply attention to value\n",
        "        output_attention = torch.bmm(proj_value, attention.permute(0, 2, 1)) # B x C x HW\n",
        "        output_attention = output_attention.view(batch_size, C, H, W)        # B x C x H x W\n",
        "\n",
        "        # Pass through an additional convolution\n",
        "        output_attention = self.post_attention_conv(output_attention)\n",
        "\n",
        "        # Residual connection with learnable gamma\n",
        "        output = self.gamma_param * output_attention + x\n",
        "        return output\n",
        "\n"
      ],
      "metadata": {
        "id": "mHZno6VXZncb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Residual Blocks**"
      ],
      "metadata": {
        "id": "k3S5W6eUbB4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GenResBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1,\n",
        "                 condition_dim=148, use_upsample=True):\n",
        "        super().__init__()\n",
        "        self.use_upsample = use_upsample\n",
        "\n",
        "        # Convolutional layers with Spectral Normalization\n",
        "        self.conv1 = SpectralNormalization(nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding))\n",
        "        self.conv2 = SpectralNormalization(nn.Conv2d(out_channels, out_channels, kernel_size, padding=padding))\n",
        "        self.conv3 = SpectralNormalization(nn.Conv2d(out_channels, out_channels, kernel_size, padding=padding))\n",
        "\n",
        "        # Adaptive Normalization layers\n",
        "        self.norm1 = AdaptiveNormalization(out_channels, condition_dim)\n",
        "        self.norm2 = AdaptiveNormalization(out_channels, condition_dim)\n",
        "\n",
        "        self.skip_projection = False\n",
        "        # 1x1 convolution for skip connection if channels change or upsampling is used\n",
        "        if in_channels != out_channels or use_upsample:\n",
        "            self.conv_skip = SpectralNormalization(nn.Conv2d(in_channels, out_channels, 1, padding=0))\n",
        "            self.skip_projection = True\n",
        "\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, input_tensor, condition_vector):\n",
        "        out = self.conv1(input_tensor)\n",
        "        out = self.activation(out)\n",
        "        out = self.norm1(out, condition_vector)\n",
        "\n",
        "        if self.use_upsample:\n",
        "            out = F.interpolate(out, scale_factor=2, mode='nearest') # Upsample\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.activation(out)\n",
        "        out = self.norm2(out, condition_vector)\n",
        "        out = self.conv3(out) # Added conv3\n",
        "\n",
        "        skip = input_tensor\n",
        "        if self.skip_projection:\n",
        "            skip = self.activation(skip) # Added activation to skip\n",
        "            if self.use_upsample:\n",
        "                skip = F.interpolate(skip, scale_factor=2, mode='nearest') # Upsample skip\n",
        "            skip = self.conv_skip(skip)\n",
        "\n",
        "        return out + skip # Add skip connection output\n",
        "\n",
        "\n",
        "class DiscResBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1,\n",
        "                 use_downsample=True):\n",
        "        super().__init__()\n",
        "        self.use_downsample = use_downsample\n",
        "\n",
        "        # Convolutional layers with Spectral Normalization\n",
        "        self.conv1 = SpectralNormalization(nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding))\n",
        "        self.conv2 = SpectralNormalization(nn.Conv2d(out_channels, out_channels, kernel_size, padding=padding))\n",
        "        self.conv3 = SpectralNormalization(nn.Conv2d(out_channels, out_channels, kernel_size, padding=padding)) # Added layer\n",
        "\n",
        "        self.skip_projection = False\n",
        "        # 1x1 convolution for skip connection if channels change or downsampling is used\n",
        "        if in_channels != out_channels or use_downsample:\n",
        "            self.conv_skip = SpectralNormalization(nn.Conv2d(in_channels, out_channels, 1, padding=0))\n",
        "            self.skip_projection = True\n",
        "\n",
        "        self.activation = nn.LeakyReLU(0.2)\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        out = self.conv1(input_tensor)\n",
        "        out = self.activation(out)\n",
        "        if self.use_downsample:\n",
        "            out = F.avg_pool2d(out, 2) # Downsample\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.activation(out)\n",
        "        out = self.conv3(out) # Added conv3\n",
        "\n",
        "        # Skip connection: input_tensor -> LeakyReLU -> conv_skip -> (downsample if needed)\n",
        "        skip = input_tensor\n",
        "        if self.skip_projection:\n",
        "            skip = self.activation(skip) # Added activation to skip\n",
        "            skip = self.conv_skip(skip)\n",
        "            if self.use_downsample:\n",
        "                skip = F.avg_pool2d(skip, 2) # Downsample skip\n",
        "\n",
        "        return out + skip # Add skip connection output\n",
        "\n"
      ],
      "metadata": {
        "id": "0RihAv3Ea8I3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Generator and Discriminator Model**"
      ],
      "metadata": {
        "id": "Q22L0WeCbGR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageGenerator(nn.Module):\n",
        "    def __init__(self, latent_dim=120, num_classes=1000, base_channels=96):\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_classes = num_classes\n",
        "        self.base_channels = base_channels\n",
        "\n",
        "        # Class embedding layer for conditioning\n",
        "        self.class_embedding_layer = SpectralNormalization(nn.Linear(num_classes, 128, bias=False))\n",
        "\n",
        "        self.initial_dense_layer = SpectralNormalization(nn.Linear(20, 8 * 8 * (16 * base_channels)))\n",
        "        self.initial_feature_map_channels = 16 * base_channels\n",
        "\n",
        "        # Generator blocks\n",
        "        self.gen_blocks = nn.ModuleList([\n",
        "            # GenResBlock without upsample for the first block if starting at 8x8\n",
        "            GenResBlock(16 * base_channels, 16 * base_channels, condition_dim=128 + 20, use_upsample=False), # 8x8 -> 8x8\n",
        "            SpatialAttentionBlock(16 * base_channels), # Attention at 8x8 (Moved earlier)\n",
        "            GenResBlock(16 * base_channels, 8 * base_channels, condition_dim=128 + 20),  # 8x8 -> 16x16\n",
        "            GenResBlock(8 * base_channels, 4 * base_channels, condition_dim=128 + 20),   # 16x16 -> 32x32\n",
        "            GenResBlock(4 * base_channels, 2 * base_channels, condition_dim=128 + 20),   # 32x32 -> 64x64\n",
        "            GenResBlock(2 * base_channels, 1 * base_channels, condition_dim=128 + 20)    # 64x64 -> 128x128\n",
        "        ])\n",
        "\n",
        "        # Final layers\n",
        "        self.final_batch_norm = nn.BatchNorm2d(1 * base_channels)\n",
        "        self.output_conv_pre_tanh = SpectralNormalization(nn.Conv2d(1 * base_channels, 3, kernel_size=3, padding=1))\n",
        "\n",
        "    def forward(self, latent_code, class_one_hot):\n",
        "        # Split latent code into parts for different blocks\n",
        "        latent_code_parts = torch.split(latent_code, 20, 1) # 120 / 20 = 6 parts\n",
        "\n",
        "        # Get class embedding\n",
        "        class_embedding = self.class_embedding_layer(class_one_hot)\n",
        "\n",
        "        # Initial projection from latent to feature map (now 8x8)\n",
        "        out = self.initial_dense_layer(latent_code_parts[0])\n",
        "        out = out.view(-1, self.initial_feature_map_channels, 8, 8) # Reshape to 8x8 feature map\n",
        "\n",
        "        latent_part_idx = 1\n",
        "        # Pass through generator blocks\n",
        "        for block in self.gen_blocks:\n",
        "            if isinstance(block, GenResBlock):\n",
        "                # Concatenate current latent part and class embedding for conditioning\n",
        "                condition_vector = torch.cat([latent_code_parts[latent_part_idx], class_embedding], 1)\n",
        "                out = block(out, condition_vector)\n",
        "                latent_part_idx += 1\n",
        "            else: # SpatialAttentionBlock\n",
        "                out = block(out)\n",
        "\n",
        "        # Final layers\n",
        "        out = self.final_batch_norm(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.output_conv_pre_tanh(out) # Pass through new final conv layer\n",
        "\n",
        "        return torch.tanh(out) # Output image in [-1, 1] range\n",
        "\n",
        "\n",
        "class ImageDiscriminator(nn.Module):\n",
        "    def __init__(self, num_classes=1000, base_channels=96):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.base_channels = base_channels\n",
        "\n",
        "        self.initial_block = nn.Sequential(\n",
        "            SpectralNormalization(nn.Conv2d(3, 1 * base_channels, kernel_size=3, padding=1)),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            SpectralNormalization(nn.Conv2d(1 * base_channels, 1 * base_channels, kernel_size=3, padding=1)),\n",
        "            nn.AvgPool2d(2)\n",
        "        )\n",
        "        # Skip connection for the initial block\n",
        "        self.initial_skip_conv = SpectralNormalization(nn.Conv2d(3, 1 * base_channels, kernel_size=1, padding=0))\n",
        "\n",
        "        self.disc_blocks = nn.Sequential(\n",
        "            DiscResBlock(1 * base_channels, 1 * base_channels, use_downsample=True), # 64x64 -> 32x32\n",
        "            DiscResBlock(1 * base_channels, 2 * base_channels, use_downsample=True), # 32x32 -> 16x16\n",
        "            SpatialAttentionBlock(2 * base_channels), # Attention at 16x16 (Moved later)\n",
        "            DiscResBlock(2 * base_channels, 4 * base_channels, use_downsample=True), # 16x16 -> 8x8\n",
        "            DiscResBlock(4 * base_channels, 8 * base_channels, use_downsample=True), # 8x8 -> 4x4\n",
        "            DiscResBlock(8 * base_channels, 16 * base_channels, use_downsample=True), # 4x4 -> 2x2\n",
        "            DiscResBlock(16 * base_channels, 16 * base_channels, use_downsample=False) # 2x2 (no downsample)\n",
        "        )\n",
        "\n",
        "        # Output linear layer for real/fake score\n",
        "        self.output_linear = SpectralNormalization(nn.Linear(16 * base_channels, 1))\n",
        "\n",
        "        # Class embedding layer for projection discriminator\n",
        "        self.class_embedding_layer = SpectralNormalization(nn.Embedding(num_classes, 16 * base_channels))\n",
        "        # Initialize embedding weights\n",
        "        self.class_embedding_layer.module.weight_bar.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "        self.activation = nn.LeakyReLU(0.2)\n",
        "\n",
        "\n",
        "    def forward(self, image, class_id_int):\n",
        "        # Initial block processing\n",
        "        out = self.initial_block(image)\n",
        "        # Add skip connection for initial block\n",
        "        out = out + self.initial_skip_conv(F.avg_pool2d(image, 2))\n",
        "\n",
        "        # Pass through discriminator blocks\n",
        "        out = self.disc_blocks(out)\n",
        "        out = F.relu(out) # Final ReLU before pooling\n",
        "\n",
        "        # Adaptive average pooling before linear layer\n",
        "        out = F.adaptive_avg_pool2d(out, 1).view(out.size(0), -1) # Global average pooling\n",
        "\n",
        "        # Get real/fake score\n",
        "        real_fake_score = self.output_linear(out).squeeze(1)\n",
        "\n",
        "        # Get class conditional score (projection discriminator)\n",
        "        class_embedding = self.class_embedding_layer(class_id_int)\n",
        "        # Apply LeakyReLU to output features before dot product for conditional score\n",
        "        class_conditional_score = (self.activation(out) * class_embedding).sum(1)\n",
        "\n",
        "        return real_fake_score + class_conditional_score # Combined score\n"
      ],
      "metadata": {
        "id": "Q2VKYJiLZakw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model Architecture**"
      ],
      "metadata": {
        "id": "Al3xq1PL9Khy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "G = ImageGenerator(latent_dim=120, num_classes = 1, base_channels=64).to('cuda')\n",
        "D = ImageDiscriminator(num_classes=1, base_channels=64).to('cuda')\n",
        "\n",
        "\n",
        "print(\"\\n--- Generator Architecture ---\")\n",
        "print(G)\n",
        "print(\"\\n--- Discriminator Architecture ---\")\n",
        "print(D)\n",
        "\n",
        "del G\n",
        "del D\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "8CPs9Luo8AXC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd5ec9f4-58c9-4ee0-8b3c-ec17d5cf2e7c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Generator Architecture ---\n",
            "ImageGenerator(\n",
            "  (class_embedding_layer): SpectralNormalization(\n",
            "    (module): Linear(in_features=1, out_features=128, bias=False)\n",
            "  )\n",
            "  (initial_dense_layer): SpectralNormalization(\n",
            "    (module): Linear(in_features=20, out_features=65536, bias=True)\n",
            "  )\n",
            "  (gen_blocks): ModuleList(\n",
            "    (0): GenResBlock(\n",
            "      (conv1): SpectralNormalization(\n",
            "        (module): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv2): SpectralNormalization(\n",
            "        (module): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv3): SpectralNormalization(\n",
            "        (module): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (norm1): AdaptiveNormalization(\n",
            "        (batch_norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
            "        (embedding_layer): Linear(in_features=148, out_features=2048, bias=True)\n",
            "      )\n",
            "      (norm2): AdaptiveNormalization(\n",
            "        (batch_norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
            "        (embedding_layer): Linear(in_features=148, out_features=2048, bias=True)\n",
            "      )\n",
            "      (activation): ReLU()\n",
            "    )\n",
            "    (1): SpatialAttentionBlock(\n",
            "      (query_conv): SpectralNormalization(\n",
            "        (module): Conv2d(1024, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (key_conv): SpectralNormalization(\n",
            "        (module): Conv2d(1024, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (value_conv): SpectralNormalization(\n",
            "        (module): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (softmax_fn): Softmax(dim=-1)\n",
            "      (post_attention_conv): SpectralNormalization(\n",
            "        (module): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (2): GenResBlock(\n",
            "      (conv1): SpectralNormalization(\n",
            "        (module): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv2): SpectralNormalization(\n",
            "        (module): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv3): SpectralNormalization(\n",
            "        (module): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (norm1): AdaptiveNormalization(\n",
            "        (batch_norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
            "        (embedding_layer): Linear(in_features=148, out_features=1024, bias=True)\n",
            "      )\n",
            "      (norm2): AdaptiveNormalization(\n",
            "        (batch_norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
            "        (embedding_layer): Linear(in_features=148, out_features=1024, bias=True)\n",
            "      )\n",
            "      (conv_skip): SpectralNormalization(\n",
            "        (module): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (activation): ReLU()\n",
            "    )\n",
            "    (3): GenResBlock(\n",
            "      (conv1): SpectralNormalization(\n",
            "        (module): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv2): SpectralNormalization(\n",
            "        (module): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv3): SpectralNormalization(\n",
            "        (module): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (norm1): AdaptiveNormalization(\n",
            "        (batch_norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
            "        (embedding_layer): Linear(in_features=148, out_features=512, bias=True)\n",
            "      )\n",
            "      (norm2): AdaptiveNormalization(\n",
            "        (batch_norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
            "        (embedding_layer): Linear(in_features=148, out_features=512, bias=True)\n",
            "      )\n",
            "      (conv_skip): SpectralNormalization(\n",
            "        (module): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (activation): ReLU()\n",
            "    )\n",
            "    (4): GenResBlock(\n",
            "      (conv1): SpectralNormalization(\n",
            "        (module): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv2): SpectralNormalization(\n",
            "        (module): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv3): SpectralNormalization(\n",
            "        (module): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (norm1): AdaptiveNormalization(\n",
            "        (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
            "        (embedding_layer): Linear(in_features=148, out_features=256, bias=True)\n",
            "      )\n",
            "      (norm2): AdaptiveNormalization(\n",
            "        (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
            "        (embedding_layer): Linear(in_features=148, out_features=256, bias=True)\n",
            "      )\n",
            "      (conv_skip): SpectralNormalization(\n",
            "        (module): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (activation): ReLU()\n",
            "    )\n",
            "    (5): GenResBlock(\n",
            "      (conv1): SpectralNormalization(\n",
            "        (module): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv2): SpectralNormalization(\n",
            "        (module): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv3): SpectralNormalization(\n",
            "        (module): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (norm1): AdaptiveNormalization(\n",
            "        (batch_norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
            "        (embedding_layer): Linear(in_features=148, out_features=128, bias=True)\n",
            "      )\n",
            "      (norm2): AdaptiveNormalization(\n",
            "        (batch_norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
            "        (embedding_layer): Linear(in_features=148, out_features=128, bias=True)\n",
            "      )\n",
            "      (conv_skip): SpectralNormalization(\n",
            "        (module): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (activation): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (final_batch_norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (output_conv_pre_tanh): SpectralNormalization(\n",
            "    (module): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            ")\n",
            "\n",
            "--- Discriminator Architecture ---\n",
            "ImageDiscriminator(\n",
            "  (initial_block): Sequential(\n",
            "    (0): SpectralNormalization(\n",
            "      (module): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    )\n",
            "    (1): LeakyReLU(negative_slope=0.2)\n",
            "    (2): SpectralNormalization(\n",
            "      (module): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    )\n",
            "    (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "  )\n",
            "  (initial_skip_conv): SpectralNormalization(\n",
            "    (module): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (disc_blocks): Sequential(\n",
            "    (0): DiscResBlock(\n",
            "      (conv1): SpectralNormalization(\n",
            "        (module): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv2): SpectralNormalization(\n",
            "        (module): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv3): SpectralNormalization(\n",
            "        (module): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv_skip): SpectralNormalization(\n",
            "        (module): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (activation): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (1): DiscResBlock(\n",
            "      (conv1): SpectralNormalization(\n",
            "        (module): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv2): SpectralNormalization(\n",
            "        (module): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv3): SpectralNormalization(\n",
            "        (module): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv_skip): SpectralNormalization(\n",
            "        (module): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (activation): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (2): SpatialAttentionBlock(\n",
            "      (query_conv): SpectralNormalization(\n",
            "        (module): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (key_conv): SpectralNormalization(\n",
            "        (module): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (value_conv): SpectralNormalization(\n",
            "        (module): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (softmax_fn): Softmax(dim=-1)\n",
            "      (post_attention_conv): SpectralNormalization(\n",
            "        (module): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (3): DiscResBlock(\n",
            "      (conv1): SpectralNormalization(\n",
            "        (module): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv2): SpectralNormalization(\n",
            "        (module): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv3): SpectralNormalization(\n",
            "        (module): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv_skip): SpectralNormalization(\n",
            "        (module): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (activation): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (4): DiscResBlock(\n",
            "      (conv1): SpectralNormalization(\n",
            "        (module): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv2): SpectralNormalization(\n",
            "        (module): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv3): SpectralNormalization(\n",
            "        (module): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv_skip): SpectralNormalization(\n",
            "        (module): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (activation): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (5): DiscResBlock(\n",
            "      (conv1): SpectralNormalization(\n",
            "        (module): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv2): SpectralNormalization(\n",
            "        (module): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv3): SpectralNormalization(\n",
            "        (module): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv_skip): SpectralNormalization(\n",
            "        (module): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (activation): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (6): DiscResBlock(\n",
            "      (conv1): SpectralNormalization(\n",
            "        (module): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv2): SpectralNormalization(\n",
            "        (module): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv3): SpectralNormalization(\n",
            "        (module): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (activation): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "  )\n",
            "  (output_linear): SpectralNormalization(\n",
            "    (module): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  )\n",
            "  (class_embedding_layer): SpectralNormalization(\n",
            "    (module): Embedding(1, 1024)\n",
            "  )\n",
            "  (activation): LeakyReLU(negative_slope=0.2)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Training the model**"
      ],
      "metadata": {
        "id": "jPrnRr2VbfxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imsize = 128\n",
        "z_dim = 120\n",
        "chn = 64 # Base channels for Generator and Discriminator\n",
        "lambda_gp = 10.0 # Gradient penalty weight\n",
        "version = 'Gan_flower102' # Version string for output directories\n",
        "total_step = 200000 # Total training steps\n",
        "d_iters = 5 # Discriminator iterations per generator iteration\n",
        "batch_size = 32\n",
        "num_workers = 2\n",
        "g_lr = 0.0001 # Generator learning rate\n",
        "d_lr = 0.0004 # Discriminator learning rate\n",
        "beta1 = 0.0 # Adam beta1\n",
        "beta2 = 0.9 # Adam beta2\n",
        "seed = 42 # Random seed\n",
        "image_path = '/content/flowers' # Path to extracted flower images\n",
        "log_path = '/content/logs' # Path for TensorBoard logs\n",
        "model_save_path = '/content/models' # Path for saving model\n",
        "sample_path = '/content/samples' # Path for saving generated samples\n",
        "attn_path = '/content/attn' # Path for attention maps (not used in this version)\n",
        "\n",
        "log_step = 100 # Frequency for logging training progress\n",
        "sample_step = 5000 # Frequency for saving generated samples\n",
        "\n",
        "pretrained_gen_path = '100000_G.pth'\n",
        "pretrained_disc_path = '100000_D.pth'\n",
        "\n",
        "#pretrained_gen_path = None\n",
        "#pretrained_disc_path = None\n",
        "\n",
        "# --- Global Setup and Directory Creation ---\n",
        "cudnn.benchmark = True # Optimize cuDNN for faster training\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True # Ensure reproducibility\n",
        "\n",
        "# Create output directories\n",
        "model_output_dir = create_output_directory(model_save_path, version)\n",
        "sample_output_dir = create_output_directory(sample_path, version)\n",
        "log_output_dir = create_output_directory(log_path, version)\n",
        "\n",
        "# Determine device for training\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Training on device: {device}\")\n",
        "\n",
        "\n",
        "# --- Data Loader Initialization ---\n",
        "# Define image transformations\n",
        "transform_list = [\n",
        "    transforms.CenterCrop(160), # Crop to a square\n",
        "    transforms.Resize((imsize, imsize)), # Resize to target size\n",
        "    transforms.ToTensor(), # Convert to tensor\n",
        "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)) # Normalize to [-1, 1]\n",
        "]\n",
        "image_transforms = transforms.Compose(transform_list)\n",
        "\n",
        "# Load dataset from image folder structure\n",
        "dataset = dsets.ImageFolder(image_path, transform=image_transforms)\n",
        "num_classes = len(dataset.classes) # Get number of classes\n",
        "\n",
        "# Create data loader\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset=dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=True # Drop the last incomplete batch\n",
        ")\n",
        "\n",
        "\n",
        "# --- Model and Optimizer Initialization---\n",
        "generator = ImageGenerator(\n",
        "    latent_dim=z_dim,\n",
        "    num_classes=num_classes,\n",
        "    base_channels=chn\n",
        ").to(device)\n",
        "\n",
        "discriminator = ImageDiscriminator(\n",
        "    num_classes=num_classes,\n",
        "    base_channels=chn\n",
        ").to(device)\n",
        "\n",
        "gen_optimizer = torch.optim.Adam(\n",
        "    filter(lambda p: p.requires_grad, generator.parameters()),\n",
        "    lr=g_lr, betas=[beta1, beta2]\n",
        ")\n",
        "disc_optimizer = torch.optim.Adam(\n",
        "    filter(lambda p: p.requires_grad, discriminator.parameters()),\n",
        "    lr=d_lr, betas=[beta1, beta2]\n",
        ")\n",
        "\n",
        "# --- Load Pre-trained Models ---\n",
        "start_step = 0\n",
        "if pretrained_gen_path:\n",
        "    gen_load_path = os.path.join(model_output_dir, pretrained_gen_path)\n",
        "    if os.path.exists(gen_load_path):\n",
        "        generator.load_state_dict(torch.load(gen_load_path, map_location=device))\n",
        "        match = re.search(r'(\\d+)_G\\.pth', pretrained_gen_path)\n",
        "        start_step = int(match.group(1))\n",
        "\n",
        "if pretrained_disc_path:\n",
        "    disc_load_path = os.path.join(model_output_dir, pretrained_disc_path)\n",
        "    if os.path.exists(disc_load_path):\n",
        "        discriminator.load_state_dict(torch.load(disc_load_path, map_location=device))\n",
        "\n",
        "# --- TensorBoard Setup ---\n",
        "tf_log_path = os.path.join(log_output_dir, 'tensorboard_logs')\n",
        "summary_writer = SummaryWriter(log_dir=tf_log_path)\n",
        "\n",
        "# --- Helper Functions for Training Loop ---\n",
        "def zero_grad_optimizers():\n",
        "    disc_optimizer.zero_grad()\n",
        "    gen_optimizer.zero_grad()\n",
        "\n",
        "def generate_random_labels(batch_size_val, num_classes_val, device_val):\n",
        "    labels_int = torch.randint(0, num_classes_val, (batch_size_val,)).to(device_val)\n",
        "    labels_one_hot = F.one_hot(labels_int, num_classes=num_classes_val).float().to(device_val)\n",
        "    return labels_int, labels_one_hot\n",
        "\n",
        "def save_real_image_sample_func(data_loader_val, sample_output_dir_val):\n",
        "    real_images, _ = next(iter(data_loader_val))\n",
        "    save_image(denormalize_image(real_images), os.path.join(sample_output_dir_val, 'real_images_sample.png'))\n",
        "\n",
        "# Save a sample of real images\n",
        "save_real_image_sample_func(data_loader, sample_output_dir)\n",
        "\n",
        "\n",
        "# --- Training Loop ---\n",
        "data_iterator = iter(data_loader)\n",
        "\n",
        "# Fixed latent code and labels for consistent sample generation\n",
        "fixed_latent_z = move_tensor_to_device(torch.randn(batch_size, z_dim), device)\n",
        "fixed_labels_int, fixed_labels_one_hot = generate_random_labels(batch_size, num_classes, device)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# History lists for plotting losses\n",
        "d_total_loss_history = []\n",
        "g_total_loss_history = []\n",
        "steps_history = []\n",
        "\n",
        "print('Starting GAN training...')\n",
        "for step in range(start_step, total_step):\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "\n",
        "    # --- Train Discriminator ---\n",
        "    # Fetch real images and labels\n",
        "    try:\n",
        "        real_images, real_labels_int = next(data_iterator)\n",
        "    except StopIteration:\n",
        "        # Reset iterator if end of dataset is reached\n",
        "        data_iterator = iter(data_loader)\n",
        "        real_images, real_labels_int = next(data_iterator)\n",
        "\n",
        "    real_images = move_tensor_to_device(real_images, device)\n",
        "    real_labels_int = move_tensor_to_device(real_labels_int, device)\n",
        "\n",
        "    # Discriminator output for real images\n",
        "    d_out_real = discriminator(real_images, real_labels_int)\n",
        "    d_loss_real = - torch.mean(d_out_real) # Maximize D(real)\n",
        "\n",
        "    # Generate fake images\n",
        "    z_latent = move_tensor_to_device(torch.randn(batch_size, z_dim), device)\n",
        "    fake_labels_int, fake_labels_one_hot = generate_random_labels(batch_size, num_classes, device)\n",
        "    fake_images = generator(z_latent, fake_labels_one_hot)\n",
        "\n",
        "    # Discriminator output for fake images\n",
        "    d_out_fake = discriminator(fake_images.detach(), fake_labels_int) # Detach fake_images to prevent G from being updated\n",
        "    d_loss_fake = d_out_fake.mean() # Minimize D(fake)\n",
        "\n",
        "    # Calculate Gradient Penalty\n",
        "    # Interpolate between real and fake images\n",
        "    alpha = torch.rand(real_images.size(0), 1, 1, 1, device=device)\n",
        "    alpha = alpha.expand_as(real_images)\n",
        "    interpolated_images = (alpha * real_images.data + (1 - alpha) * fake_images.data).requires_grad_(True)\n",
        "\n",
        "    # Discriminator output for interpolated images\n",
        "    d_out_interpolated = discriminator(interpolated_images, real_labels_int)\n",
        "\n",
        "    # Compute gradients of D_out_interpolated with respect to interpolated_images\n",
        "    gradients = torch.autograd.grad(\n",
        "        outputs=d_out_interpolated,\n",
        "        inputs=interpolated_images,\n",
        "        grad_outputs=torch.ones_like(d_out_interpolated, device=device),\n",
        "        retain_graph=True,\n",
        "        create_graph=True,\n",
        "        only_inputs=True\n",
        "    )[0]\n",
        "\n",
        "    # Calculate gradient norm and penalty\n",
        "    gradients = gradients.view(gradients.size(0), -1)\n",
        "    grad_norm = gradients.norm(2, dim=1)\n",
        "    d_loss_gp = torch.mean((grad_norm - 1) ** 2)\n",
        "\n",
        "    # Total Discriminator Loss\n",
        "    total_d_loss = d_loss_real + d_loss_fake + lambda_gp * d_loss_gp\n",
        "\n",
        "    # Backward pass and optimize Discriminator\n",
        "    zero_grad_optimizers()\n",
        "    total_d_loss.backward()\n",
        "    disc_optimizer.step()\n",
        "\n",
        "    # --- Train Generator (every d_iters steps) ---\n",
        "    if (step + 1) % d_iters == 0:\n",
        "        # Generate new fake images\n",
        "        z_latent = move_tensor_to_device(torch.randn(batch_size, z_dim), device)\n",
        "        fake_labels_int, fake_labels_one_hot = generate_random_labels(batch_size, num_classes, device)\n",
        "\n",
        "        fake_images = generator(z_latent, fake_labels_one_hot)\n",
        "        g_out_fake = discriminator(fake_images, fake_labels_int) # D(G(z))\n",
        "\n",
        "        gen_loss = - g_out_fake.mean() # Maximize D(G(z))\n",
        "\n",
        "        # Backward pass and optimize Generator\n",
        "        zero_grad_optimizers()\n",
        "        gen_loss.backward()\n",
        "        gen_optimizer.step()\n",
        "\n",
        "        # --- Logging and Monitoring ---\n",
        "        if (step + 1) % log_step == 0:\n",
        "            elapsed_time = time.time() - start_time\n",
        "            elapsed_time_str = str(datetime.timedelta(seconds=elapsed_time))\n",
        "            log_message = (\n",
        "                f\"Step [{step + 1}/{total_step}], \"\n",
        "                f\"D_loss_real: {d_loss_real.item():.4f}, D_loss_fake: {d_loss_fake.item():.4f}, \"\n",
        "            )\n",
        "            log_message += f\"D_loss_gp: {d_loss_gp.item():.4f}, \"\n",
        "            log_message += f\"G_loss: {gen_loss.item():.4f}\"\n",
        "            print(log_message)\n",
        "\n",
        "            steps_history.append(step)\n",
        "            d_total_loss_history.append(total_d_loss.item())\n",
        "            g_total_loss_history.append(gen_loss.item())\n",
        "\n",
        "            # Log to TensorBoard\n",
        "            summary_writer.add_scalar('Loss/D_real', d_loss_real.item(), (step + 1))\n",
        "            summary_writer.add_scalar('Loss/D_fake', d_loss_fake.item(), (step + 1))\n",
        "            summary_writer.add_scalar('Loss/D_total', total_d_loss.item(), (step + 1))\n",
        "            summary_writer.add_scalar('Loss/D_gp', d_loss_gp.item(), (step + 1))\n",
        "            summary_writer.add_scalar('Loss/G_total', gen_loss.item(), (step + 1))\n",
        "\n",
        "        # --- Save Sample Images ---\n",
        "        if (step + 1) % sample_step == 0:\n",
        "            generator.eval() # Set generator to evaluation mode\n",
        "            with torch.no_grad(): # Disable gradient calculation\n",
        "                generated_samples = generator(fixed_latent_z, fixed_labels_one_hot)\n",
        "            save_image(denormalize_image(generated_samples.data),\n",
        "                       os.path.join(sample_output_dir, f'{step + 1}_generated.png'))\n",
        "            generator.train() # Set generator back to training mode\n",
        "\n",
        "# Save final models after training completes\n",
        "torch.save(generator.state_dict(), os.path.join(model_output_dir, f'{total_step}_G.pth'))\n",
        "torch.save(discriminator.state_dict(), os.path.join(model_output_dir, f'{total_step}_D.pth'))\n",
        "\n",
        "\n",
        "# Close TensorBoard writer\n",
        "summary_writer.close()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ehkpeyQvYNpQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "faf3cd23-d92f-42df-a806-df03700461ce"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on device: cuda\n",
            "Starting GAN training...\n",
            "Step [100100/200000], D_loss_real: -3502.6516, D_loss_fake: 3502.4717, D_loss_gp: 0.0048, G_loss: -3475.3096\n",
            "Step [100200/200000], D_loss_real: -3225.1311, D_loss_fake: 3222.5388, D_loss_gp: 0.0195, G_loss: -3300.8660\n",
            "Step [100300/200000], D_loss_real: -2534.8735, D_loss_fake: 2531.3931, D_loss_gp: 0.0382, G_loss: -2533.1965\n",
            "Step [100400/200000], D_loss_real: -1940.4595, D_loss_fake: 1944.1321, D_loss_gp: 0.0533, G_loss: -1974.2791\n",
            "Step [100500/200000], D_loss_real: -1117.1329, D_loss_fake: 1116.0376, D_loss_gp: 0.0248, G_loss: -1168.6519\n",
            "Step [100600/200000], D_loss_real: -2898.5308, D_loss_fake: 2896.0227, D_loss_gp: 0.0167, G_loss: -2999.8657\n",
            "Step [100700/200000], D_loss_real: -2588.0010, D_loss_fake: 2585.8208, D_loss_gp: 0.0111, G_loss: -2580.2551\n",
            "Step [100800/200000], D_loss_real: -2103.0059, D_loss_fake: 2103.6592, D_loss_gp: 0.0257, G_loss: -2214.3633\n",
            "Step [100900/200000], D_loss_real: -3892.9695, D_loss_fake: 3895.2832, D_loss_gp: 0.0154, G_loss: -3866.0542\n",
            "Step [101000/200000], D_loss_real: -4017.8911, D_loss_fake: 4010.4500, D_loss_gp: 0.0324, G_loss: -4085.8125\n",
            "Step [101100/200000], D_loss_real: -4588.3389, D_loss_fake: 4590.9316, D_loss_gp: 0.0122, G_loss: -4506.0952\n",
            "Step [101200/200000], D_loss_real: -4747.5171, D_loss_fake: 4741.0093, D_loss_gp: 0.0270, G_loss: -4830.8047\n",
            "Step [101300/200000], D_loss_real: -4388.6299, D_loss_fake: 4389.5605, D_loss_gp: 0.0179, G_loss: -4344.8716\n",
            "Step [101400/200000], D_loss_real: -2722.5718, D_loss_fake: 2730.7996, D_loss_gp: 0.0068, G_loss: -2511.0696\n",
            "Step [101500/200000], D_loss_real: -1086.5503, D_loss_fake: 1084.1458, D_loss_gp: 0.0292, G_loss: -1216.9185\n",
            "Step [101600/200000], D_loss_real: -1411.2222, D_loss_fake: 1408.7362, D_loss_gp: 0.0691, G_loss: -1235.2043\n",
            "Step [101700/200000], D_loss_real: -1335.1509, D_loss_fake: 1332.7786, D_loss_gp: 0.0282, G_loss: -1360.0443\n",
            "Step [101800/200000], D_loss_real: -1333.7773, D_loss_fake: 1335.1370, D_loss_gp: 0.0288, G_loss: -1302.3052\n",
            "Step [101900/200000], D_loss_real: -1397.3158, D_loss_fake: 1393.9989, D_loss_gp: 0.0186, G_loss: -1406.7946\n",
            "Step [102000/200000], D_loss_real: -619.0751, D_loss_fake: 616.2222, D_loss_gp: 0.0345, G_loss: -627.6194\n",
            "Step [102100/200000], D_loss_real: -3166.6577, D_loss_fake: 3164.9346, D_loss_gp: 0.0583, G_loss: -3135.1616\n",
            "Step [102200/200000], D_loss_real: -2812.1489, D_loss_fake: 2809.6484, D_loss_gp: 0.0274, G_loss: -2844.3555\n",
            "Step [102300/200000], D_loss_real: -2956.0474, D_loss_fake: 2955.2383, D_loss_gp: 0.0180, G_loss: -2987.8140\n",
            "Step [102400/200000], D_loss_real: -423.0414, D_loss_fake: 418.9610, D_loss_gp: 0.0536, G_loss: -600.5804\n",
            "Step [102500/200000], D_loss_real: -2752.5103, D_loss_fake: 2745.4771, D_loss_gp: 0.0772, G_loss: -2758.0742\n",
            "Step [102600/200000], D_loss_real: -2229.1709, D_loss_fake: 2229.5850, D_loss_gp: 0.0370, G_loss: -2260.7185\n",
            "Step [102700/200000], D_loss_real: -1688.2971, D_loss_fake: 1689.8064, D_loss_gp: 0.0531, G_loss: -1629.1123\n",
            "Step [102800/200000], D_loss_real: -2526.5781, D_loss_fake: 2525.7458, D_loss_gp: 0.0130, G_loss: -2527.0771\n",
            "Step [102900/200000], D_loss_real: -2126.7183, D_loss_fake: 2126.5735, D_loss_gp: 0.0162, G_loss: -2193.2400\n",
            "Step [103000/200000], D_loss_real: -3032.6357, D_loss_fake: 3031.0840, D_loss_gp: 0.0164, G_loss: -3098.8342\n",
            "Step [103100/200000], D_loss_real: -656.4647, D_loss_fake: 656.6938, D_loss_gp: 0.0242, G_loss: -732.7802\n",
            "Step [103200/200000], D_loss_real: -2770.9287, D_loss_fake: 2768.0427, D_loss_gp: 0.0384, G_loss: -2828.4253\n",
            "Step [103300/200000], D_loss_real: -977.7370, D_loss_fake: 977.7830, D_loss_gp: 0.0179, G_loss: -835.5023\n",
            "Step [103400/200000], D_loss_real: -3065.3743, D_loss_fake: 3064.9985, D_loss_gp: 0.0356, G_loss: -3038.3352\n",
            "Step [103500/200000], D_loss_real: -3147.5278, D_loss_fake: 3141.9575, D_loss_gp: 0.0764, G_loss: -3151.2156\n",
            "Step [103600/200000], D_loss_real: -3012.3511, D_loss_fake: 3015.0938, D_loss_gp: 0.0182, G_loss: -2936.2219\n",
            "Step [103700/200000], D_loss_real: -3108.4387, D_loss_fake: 3115.4570, D_loss_gp: 0.0712, G_loss: -3072.1929\n",
            "Step [103800/200000], D_loss_real: -2866.3247, D_loss_fake: 2871.8806, D_loss_gp: 0.0449, G_loss: -2830.7087\n",
            "Step [103900/200000], D_loss_real: -2524.2668, D_loss_fake: 2518.6582, D_loss_gp: 0.0064, G_loss: -2604.6938\n",
            "Step [104000/200000], D_loss_real: -1960.3403, D_loss_fake: 1962.3271, D_loss_gp: 0.0126, G_loss: -1905.1500\n",
            "Step [104100/200000], D_loss_real: -1136.6047, D_loss_fake: 1137.4677, D_loss_gp: 0.0142, G_loss: -1121.4119\n",
            "Step [104200/200000], D_loss_real: -699.9244, D_loss_fake: 696.8424, D_loss_gp: 0.0359, G_loss: -784.4033\n",
            "Step [104300/200000], D_loss_real: -2339.4780, D_loss_fake: 2343.1943, D_loss_gp: 0.0234, G_loss: -2239.9670\n",
            "Step [104400/200000], D_loss_real: -2679.5923, D_loss_fake: 2675.1382, D_loss_gp: 0.0153, G_loss: -2775.9956\n",
            "Step [104500/200000], D_loss_real: -2867.6152, D_loss_fake: 2871.9766, D_loss_gp: 0.0279, G_loss: -2837.9993\n",
            "Step [104600/200000], D_loss_real: -3161.7241, D_loss_fake: 3162.5449, D_loss_gp: 0.0442, G_loss: -3179.9961\n",
            "Step [104700/200000], D_loss_real: -3430.0330, D_loss_fake: 3421.4343, D_loss_gp: 0.0764, G_loss: -3498.7114\n",
            "Step [104800/200000], D_loss_real: -2031.7329, D_loss_fake: 2034.3612, D_loss_gp: 0.1082, G_loss: -1924.4797\n",
            "Step [104900/200000], D_loss_real: -2703.4653, D_loss_fake: 2695.7930, D_loss_gp: 0.0669, G_loss: -2746.1284\n",
            "Step [105000/200000], D_loss_real: -2594.1118, D_loss_fake: 2591.9966, D_loss_gp: 0.1507, G_loss: -2639.2515\n",
            "Step [105100/200000], D_loss_real: -2476.7461, D_loss_fake: 2470.0840, D_loss_gp: 0.0102, G_loss: -2677.6052\n",
            "Step [105200/200000], D_loss_real: -3190.3872, D_loss_fake: 3192.3672, D_loss_gp: 1.9376, G_loss: -3211.5291\n",
            "Step [105300/200000], D_loss_real: -3297.2256, D_loss_fake: 3288.8701, D_loss_gp: 0.0342, G_loss: -3440.6675\n",
            "Step [105400/200000], D_loss_real: -3330.4517, D_loss_fake: 3326.8267, D_loss_gp: 0.0309, G_loss: -3424.3218\n",
            "Step [105500/200000], D_loss_real: -3335.6875, D_loss_fake: 3325.1377, D_loss_gp: 0.1238, G_loss: -3313.4543\n",
            "Step [105600/200000], D_loss_real: -3421.5610, D_loss_fake: 3414.1023, D_loss_gp: 0.2666, G_loss: -3467.7727\n",
            "Step [105700/200000], D_loss_real: -2896.0874, D_loss_fake: 2898.4053, D_loss_gp: 0.1447, G_loss: -2719.4412\n",
            "Step [105800/200000], D_loss_real: -3146.6160, D_loss_fake: 3156.9146, D_loss_gp: 0.0160, G_loss: -2945.0825\n",
            "Step [105900/200000], D_loss_real: -2850.7085, D_loss_fake: 2850.3286, D_loss_gp: 0.0315, G_loss: -2855.5693\n",
            "Step [106000/200000], D_loss_real: -3243.0217, D_loss_fake: 3250.5439, D_loss_gp: 0.0304, G_loss: -3213.0498\n",
            "Step [106100/200000], D_loss_real: -3952.0698, D_loss_fake: 3951.0737, D_loss_gp: 0.0765, G_loss: -3855.8438\n",
            "Step [106200/200000], D_loss_real: -2067.5784, D_loss_fake: 2065.8125, D_loss_gp: 0.0407, G_loss: -2087.3340\n",
            "Step [106300/200000], D_loss_real: -2143.4199, D_loss_fake: 2142.5767, D_loss_gp: 0.0207, G_loss: -2124.8755\n",
            "Step [106400/200000], D_loss_real: -2742.3608, D_loss_fake: 2750.4014, D_loss_gp: 0.0385, G_loss: -2569.8767\n",
            "Step [106500/200000], D_loss_real: -1271.8999, D_loss_fake: 1269.0249, D_loss_gp: 0.0851, G_loss: -1326.5132\n",
            "Step [106600/200000], D_loss_real: -2428.8115, D_loss_fake: 2424.7632, D_loss_gp: 0.0293, G_loss: -2551.1914\n",
            "Step [106700/200000], D_loss_real: -986.5336, D_loss_fake: 987.5660, D_loss_gp: 0.0396, G_loss: -961.5354\n",
            "Step [106800/200000], D_loss_real: -1881.3046, D_loss_fake: 1878.6360, D_loss_gp: 0.1134, G_loss: -1915.9766\n",
            "Step [106900/200000], D_loss_real: -2295.3799, D_loss_fake: 2303.8596, D_loss_gp: 0.0081, G_loss: -2155.1108\n",
            "Step [107000/200000], D_loss_real: -417.6270, D_loss_fake: 416.6231, D_loss_gp: 0.0464, G_loss: -393.5313\n",
            "Step [107100/200000], D_loss_real: 795.0397, D_loss_fake: -793.6608, D_loss_gp: 0.0467, G_loss: 884.1871\n",
            "Step [107200/200000], D_loss_real: -1158.2551, D_loss_fake: 1155.1658, D_loss_gp: 0.0371, G_loss: -1263.8412\n",
            "Step [107300/200000], D_loss_real: -1813.8937, D_loss_fake: 1817.8875, D_loss_gp: 0.0174, G_loss: -1724.7642\n",
            "Step [107400/200000], D_loss_real: -2345.9014, D_loss_fake: 2342.9243, D_loss_gp: 0.0311, G_loss: -2523.8286\n",
            "Step [107500/200000], D_loss_real: -2115.0432, D_loss_fake: 2118.5322, D_loss_gp: 0.0804, G_loss: -2047.0649\n",
            "Step [107600/200000], D_loss_real: -2995.0269, D_loss_fake: 2983.4497, D_loss_gp: 0.0961, G_loss: -3092.0898\n",
            "Step [107700/200000], D_loss_real: -2916.1165, D_loss_fake: 2914.2280, D_loss_gp: 0.0200, G_loss: -2957.9907\n",
            "Step [107800/200000], D_loss_real: -3165.5266, D_loss_fake: 3165.3838, D_loss_gp: 0.0158, G_loss: -3153.9380\n",
            "Step [107900/200000], D_loss_real: -4073.5352, D_loss_fake: 4071.0811, D_loss_gp: 0.0121, G_loss: -4077.5354\n",
            "Step [108000/200000], D_loss_real: -4302.3481, D_loss_fake: 4296.4775, D_loss_gp: 0.0211, G_loss: -4374.9414\n",
            "Step [108100/200000], D_loss_real: -4123.1494, D_loss_fake: 4124.3101, D_loss_gp: 0.0327, G_loss: -4114.2935\n",
            "Step [108200/200000], D_loss_real: -4131.5718, D_loss_fake: 4130.7236, D_loss_gp: 0.0714, G_loss: -4205.7148\n",
            "Step [108300/200000], D_loss_real: -3829.5852, D_loss_fake: 3822.9185, D_loss_gp: 0.0288, G_loss: -3850.7354\n",
            "Step [108400/200000], D_loss_real: -3737.2153, D_loss_fake: 3737.9377, D_loss_gp: 0.0137, G_loss: -3683.6077\n",
            "Step [108500/200000], D_loss_real: -3252.3970, D_loss_fake: 3246.5378, D_loss_gp: 0.0069, G_loss: -3358.9067\n",
            "Step [108600/200000], D_loss_real: -3887.8242, D_loss_fake: 3890.6816, D_loss_gp: 0.0620, G_loss: -3830.8369\n",
            "Step [108700/200000], D_loss_real: -3188.3662, D_loss_fake: 3184.3599, D_loss_gp: 0.0625, G_loss: -3259.7288\n",
            "Step [108800/200000], D_loss_real: -3986.2803, D_loss_fake: 3983.2983, D_loss_gp: 0.0424, G_loss: -4111.3027\n",
            "Step [108900/200000], D_loss_real: -3208.2563, D_loss_fake: 3203.9971, D_loss_gp: 0.0624, G_loss: -3298.0247\n",
            "Step [109000/200000], D_loss_real: -3196.7920, D_loss_fake: 3191.2578, D_loss_gp: 0.0230, G_loss: -3204.6487\n",
            "Step [109100/200000], D_loss_real: -3620.1140, D_loss_fake: 3610.5413, D_loss_gp: 0.0153, G_loss: -3739.4707\n",
            "Step [109200/200000], D_loss_real: -3861.9888, D_loss_fake: 3838.5732, D_loss_gp: 0.0337, G_loss: -3982.1643\n",
            "Step [109300/200000], D_loss_real: -3955.4451, D_loss_fake: 3955.5112, D_loss_gp: 0.0403, G_loss: -3998.5923\n",
            "Step [109400/200000], D_loss_real: -3672.7373, D_loss_fake: 3661.4751, D_loss_gp: 0.0366, G_loss: -3772.0300\n",
            "Step [109500/200000], D_loss_real: -3783.4321, D_loss_fake: 3777.6030, D_loss_gp: 0.0539, G_loss: -3792.0085\n",
            "Step [109600/200000], D_loss_real: -3397.4255, D_loss_fake: 3397.0049, D_loss_gp: 0.0225, G_loss: -3407.6243\n",
            "Step [109700/200000], D_loss_real: -3246.2446, D_loss_fake: 3241.7722, D_loss_gp: 0.0169, G_loss: -3270.9102\n",
            "Step [109800/200000], D_loss_real: -3071.2361, D_loss_fake: 3067.2917, D_loss_gp: 0.0500, G_loss: -3123.0024\n",
            "Step [109900/200000], D_loss_real: -2868.1763, D_loss_fake: 2870.4788, D_loss_gp: 0.0074, G_loss: -2838.0413\n",
            "Step [110000/200000], D_loss_real: -2935.8533, D_loss_fake: 2934.3513, D_loss_gp: 0.0268, G_loss: -2978.2754\n",
            "Step [110100/200000], D_loss_real: -3227.1865, D_loss_fake: 3232.7776, D_loss_gp: 0.0290, G_loss: -3152.4395\n",
            "Step [110200/200000], D_loss_real: -2390.8931, D_loss_fake: 2390.1406, D_loss_gp: 0.0058, G_loss: -2412.6631\n",
            "Step [110300/200000], D_loss_real: -1896.3804, D_loss_fake: 1896.0017, D_loss_gp: 0.0716, G_loss: -1899.6398\n",
            "Step [110400/200000], D_loss_real: -1459.6276, D_loss_fake: 1461.1470, D_loss_gp: 0.0165, G_loss: -1454.7729\n",
            "Step [110500/200000], D_loss_real: -1775.3374, D_loss_fake: 1778.8566, D_loss_gp: 0.0614, G_loss: -1757.6989\n",
            "Step [110600/200000], D_loss_real: -2541.3865, D_loss_fake: 2541.0864, D_loss_gp: 0.0156, G_loss: -2483.4109\n",
            "Step [110700/200000], D_loss_real: -2940.1143, D_loss_fake: 2929.9316, D_loss_gp: 0.0205, G_loss: -3078.8057\n",
            "Step [110800/200000], D_loss_real: -2506.5645, D_loss_fake: 2510.6091, D_loss_gp: 0.0758, G_loss: -2451.7820\n",
            "Step [110900/200000], D_loss_real: -2661.7788, D_loss_fake: 2664.9277, D_loss_gp: 0.0147, G_loss: -2666.5791\n",
            "Step [111000/200000], D_loss_real: -2017.8752, D_loss_fake: 2011.2021, D_loss_gp: 0.0813, G_loss: -2043.0952\n",
            "Step [111100/200000], D_loss_real: -2230.7556, D_loss_fake: 2227.6501, D_loss_gp: 0.0385, G_loss: -2366.0303\n",
            "Step [111200/200000], D_loss_real: -2008.5457, D_loss_fake: 2008.6675, D_loss_gp: 0.0252, G_loss: -1968.6072\n",
            "Step [111300/200000], D_loss_real: -1400.6537, D_loss_fake: 1397.9961, D_loss_gp: 0.0281, G_loss: -1372.0260\n",
            "Step [111400/200000], D_loss_real: -588.6681, D_loss_fake: 590.9833, D_loss_gp: 0.0277, G_loss: -671.2969\n",
            "Step [111500/200000], D_loss_real: -1919.9885, D_loss_fake: 1917.8533, D_loss_gp: 0.0264, G_loss: -1901.4952\n",
            "Step [111600/200000], D_loss_real: -2034.4526, D_loss_fake: 2039.3448, D_loss_gp: 0.0088, G_loss: -1984.8972\n",
            "Step [111700/200000], D_loss_real: -2242.0146, D_loss_fake: 2237.6206, D_loss_gp: 0.0278, G_loss: -2218.8486\n",
            "Step [111800/200000], D_loss_real: -1052.8403, D_loss_fake: 1051.8986, D_loss_gp: 0.0164, G_loss: -995.7741\n",
            "Step [111900/200000], D_loss_real: -1007.5425, D_loss_fake: 1005.1487, D_loss_gp: 0.0486, G_loss: -915.6420\n",
            "Step [112000/200000], D_loss_real: -943.6832, D_loss_fake: 944.9714, D_loss_gp: 0.0430, G_loss: -955.2505\n",
            "Step [112100/200000], D_loss_real: -1167.8567, D_loss_fake: 1165.0255, D_loss_gp: 0.0807, G_loss: -1144.7568\n",
            "Step [112200/200000], D_loss_real: -2062.7209, D_loss_fake: 2055.6660, D_loss_gp: 0.0470, G_loss: -2081.7930\n",
            "Step [112300/200000], D_loss_real: -1899.8354, D_loss_fake: 1895.9407, D_loss_gp: 0.0678, G_loss: -1975.1356\n",
            "Step [112400/200000], D_loss_real: -2590.2725, D_loss_fake: 2586.9683, D_loss_gp: 0.0204, G_loss: -2639.0410\n",
            "Step [112500/200000], D_loss_real: -2234.6936, D_loss_fake: 2235.2854, D_loss_gp: 0.0751, G_loss: -2219.7126\n",
            "Step [112600/200000], D_loss_real: -1646.1799, D_loss_fake: 1651.5757, D_loss_gp: 0.0525, G_loss: -1629.2819\n",
            "Step [112700/200000], D_loss_real: -196.7013, D_loss_fake: 193.9417, D_loss_gp: 0.0212, G_loss: -207.5956\n",
            "Step [112800/200000], D_loss_real: -1010.5822, D_loss_fake: 1009.6444, D_loss_gp: 0.0781, G_loss: -1057.9874\n",
            "Step [112900/200000], D_loss_real: -905.0756, D_loss_fake: 908.2560, D_loss_gp: 0.0174, G_loss: -892.8566\n",
            "Step [113000/200000], D_loss_real: -969.3826, D_loss_fake: 970.5172, D_loss_gp: 0.0249, G_loss: -946.4034\n",
            "Step [113100/200000], D_loss_real: -455.2722, D_loss_fake: 457.1032, D_loss_gp: 0.0089, G_loss: -438.2500\n",
            "Step [113200/200000], D_loss_real: -881.1561, D_loss_fake: 882.4487, D_loss_gp: 0.0343, G_loss: -914.1643\n",
            "Step [113300/200000], D_loss_real: -1217.9167, D_loss_fake: 1217.1731, D_loss_gp: 0.0321, G_loss: -1192.0078\n",
            "Step [113400/200000], D_loss_real: -750.9132, D_loss_fake: 748.7053, D_loss_gp: 0.0182, G_loss: -763.9099\n",
            "Step [113500/200000], D_loss_real: -212.8480, D_loss_fake: 212.7688, D_loss_gp: 0.0391, G_loss: -147.1862\n",
            "Step [113600/200000], D_loss_real: -711.8093, D_loss_fake: 710.3027, D_loss_gp: 0.0604, G_loss: -756.8235\n",
            "Step [113700/200000], D_loss_real: -462.6295, D_loss_fake: 461.7064, D_loss_gp: 0.0388, G_loss: -432.3030\n",
            "Step [113800/200000], D_loss_real: -456.5905, D_loss_fake: 462.0918, D_loss_gp: 0.1302, G_loss: -503.9338\n",
            "Step [113900/200000], D_loss_real: -159.4689, D_loss_fake: 156.6965, D_loss_gp: 0.0334, G_loss: -119.5811\n",
            "Step [114000/200000], D_loss_real: -912.5490, D_loss_fake: 918.4539, D_loss_gp: 0.0227, G_loss: -865.3610\n",
            "Step [114100/200000], D_loss_real: -1030.3795, D_loss_fake: 1029.3510, D_loss_gp: 0.0341, G_loss: -1006.0679\n",
            "Step [114200/200000], D_loss_real: -543.3339, D_loss_fake: 545.5976, D_loss_gp: 0.0182, G_loss: -536.5897\n",
            "Step [114300/200000], D_loss_real: -556.7296, D_loss_fake: 558.2352, D_loss_gp: 0.1142, G_loss: -622.9285\n",
            "Step [114400/200000], D_loss_real: -647.0942, D_loss_fake: 645.4413, D_loss_gp: 0.0451, G_loss: -613.0596\n",
            "Step [114500/200000], D_loss_real: -636.4237, D_loss_fake: 637.6506, D_loss_gp: 0.0337, G_loss: -656.0491\n",
            "Step [114600/200000], D_loss_real: -1130.1912, D_loss_fake: 1133.6091, D_loss_gp: 0.0402, G_loss: -1108.0182\n",
            "Step [114700/200000], D_loss_real: -1229.1676, D_loss_fake: 1232.3046, D_loss_gp: 0.2080, G_loss: -1205.4105\n",
            "Step [114800/200000], D_loss_real: -845.3363, D_loss_fake: 843.4081, D_loss_gp: 0.0704, G_loss: -799.9402\n",
            "Step [114900/200000], D_loss_real: -725.7374, D_loss_fake: 728.6226, D_loss_gp: 0.0358, G_loss: -806.4642\n",
            "Step [115000/200000], D_loss_real: -752.7084, D_loss_fake: 754.1736, D_loss_gp: 0.1023, G_loss: -738.5022\n",
            "Step [115100/200000], D_loss_real: -608.4838, D_loss_fake: 607.6880, D_loss_gp: 0.0243, G_loss: -710.1979\n",
            "Step [115200/200000], D_loss_real: -350.2728, D_loss_fake: 347.5139, D_loss_gp: 0.0196, G_loss: -339.4982\n",
            "Step [115300/200000], D_loss_real: -1902.5259, D_loss_fake: 1893.0470, D_loss_gp: 0.0921, G_loss: -1931.5698\n",
            "Step [115400/200000], D_loss_real: -1143.2119, D_loss_fake: 1145.0098, D_loss_gp: 0.0704, G_loss: -1130.8698\n",
            "Step [115500/200000], D_loss_real: -1333.5635, D_loss_fake: 1346.5215, D_loss_gp: 0.0466, G_loss: -1275.7628\n",
            "Step [115600/200000], D_loss_real: -2013.8571, D_loss_fake: 2006.3262, D_loss_gp: 0.0471, G_loss: -2047.5674\n",
            "Step [115700/200000], D_loss_real: -2311.4893, D_loss_fake: 2310.4863, D_loss_gp: 0.0668, G_loss: -2345.6448\n",
            "Step [115800/200000], D_loss_real: -2690.3818, D_loss_fake: 2693.8691, D_loss_gp: 0.0747, G_loss: -2660.7246\n",
            "Step [115900/200000], D_loss_real: -2898.0449, D_loss_fake: 2898.0015, D_loss_gp: 0.1134, G_loss: -2959.7949\n",
            "Step [116000/200000], D_loss_real: -2781.2437, D_loss_fake: 2790.7114, D_loss_gp: 0.0685, G_loss: -2686.8362\n",
            "Step [116100/200000], D_loss_real: -2676.8223, D_loss_fake: 2677.1348, D_loss_gp: 0.0694, G_loss: -2693.1157\n",
            "Step [116200/200000], D_loss_real: -3309.9448, D_loss_fake: 3308.4497, D_loss_gp: 0.0197, G_loss: -3326.7400\n",
            "Step [116300/200000], D_loss_real: -2710.9448, D_loss_fake: 2709.2402, D_loss_gp: 0.0637, G_loss: -2759.1736\n",
            "Step [116400/200000], D_loss_real: -2841.4229, D_loss_fake: 2845.5239, D_loss_gp: 0.0354, G_loss: -2788.6748\n",
            "Step [116500/200000], D_loss_real: -2491.0913, D_loss_fake: 2494.8784, D_loss_gp: 0.1007, G_loss: -2463.1792\n",
            "Step [116600/200000], D_loss_real: -3291.7090, D_loss_fake: 3289.1924, D_loss_gp: 0.0183, G_loss: -3318.1492\n",
            "Step [116700/200000], D_loss_real: -3481.9622, D_loss_fake: 3479.8511, D_loss_gp: 0.0169, G_loss: -3504.6606\n",
            "Step [116800/200000], D_loss_real: -3895.1309, D_loss_fake: 3892.8992, D_loss_gp: 0.0676, G_loss: -3996.2144\n",
            "Step [116900/200000], D_loss_real: -4308.5615, D_loss_fake: 4314.1675, D_loss_gp: 0.0286, G_loss: -4203.0312\n",
            "Step [117000/200000], D_loss_real: -3627.1519, D_loss_fake: 3627.4902, D_loss_gp: 0.0134, G_loss: -3627.4072\n",
            "Step [117100/200000], D_loss_real: -4167.5283, D_loss_fake: 4154.7842, D_loss_gp: 0.0125, G_loss: -4232.1719\n",
            "Step [117200/200000], D_loss_real: -3535.7861, D_loss_fake: 3535.2112, D_loss_gp: 0.0701, G_loss: -3601.8828\n",
            "Step [117300/200000], D_loss_real: -3208.9390, D_loss_fake: 3206.0933, D_loss_gp: 0.0170, G_loss: -3242.3110\n",
            "Step [117400/200000], D_loss_real: -2342.3738, D_loss_fake: 2339.4624, D_loss_gp: 0.0432, G_loss: -2314.5903\n",
            "Step [117500/200000], D_loss_real: -1872.2233, D_loss_fake: 1869.4441, D_loss_gp: 0.0378, G_loss: -1897.8129\n",
            "Step [117600/200000], D_loss_real: -2056.3667, D_loss_fake: 2061.0559, D_loss_gp: 0.0554, G_loss: -1972.0521\n",
            "Step [117700/200000], D_loss_real: -339.0833, D_loss_fake: 341.7011, D_loss_gp: 0.0315, G_loss: -335.9244\n",
            "Step [117800/200000], D_loss_real: -430.6825, D_loss_fake: 429.3890, D_loss_gp: 0.0268, G_loss: -451.8683\n",
            "Step [117900/200000], D_loss_real: -1303.5164, D_loss_fake: 1305.4929, D_loss_gp: 0.0620, G_loss: -1267.7100\n",
            "Step [118000/200000], D_loss_real: -709.4405, D_loss_fake: 710.3298, D_loss_gp: 0.0672, G_loss: -760.6340\n",
            "Step [118100/200000], D_loss_real: -696.1459, D_loss_fake: 695.0564, D_loss_gp: 0.0436, G_loss: -708.6503\n",
            "Step [118200/200000], D_loss_real: -1163.6927, D_loss_fake: 1162.2698, D_loss_gp: 0.0255, G_loss: -1228.5100\n",
            "Step [118300/200000], D_loss_real: -1038.3350, D_loss_fake: 1038.7196, D_loss_gp: 0.0829, G_loss: -1006.4882\n",
            "Step [118400/200000], D_loss_real: -831.9348, D_loss_fake: 830.0895, D_loss_gp: 0.0234, G_loss: -801.8743\n",
            "Step [118500/200000], D_loss_real: -506.0052, D_loss_fake: 508.2261, D_loss_gp: 0.0716, G_loss: -498.6837\n",
            "Step [118600/200000], D_loss_real: -1262.8872, D_loss_fake: 1263.3474, D_loss_gp: 0.0679, G_loss: -1213.5092\n",
            "Step [118700/200000], D_loss_real: -680.5722, D_loss_fake: 680.1494, D_loss_gp: 0.0420, G_loss: -696.8032\n",
            "Step [118800/200000], D_loss_real: -1701.0677, D_loss_fake: 1693.4617, D_loss_gp: 0.0336, G_loss: -1725.4399\n",
            "Step [118900/200000], D_loss_real: -919.5116, D_loss_fake: 920.5092, D_loss_gp: 0.1359, G_loss: -925.4829\n",
            "Step [119000/200000], D_loss_real: -1071.7561, D_loss_fake: 1068.7341, D_loss_gp: 0.0287, G_loss: -1048.4978\n",
            "Step [119100/200000], D_loss_real: -712.2466, D_loss_fake: 715.7384, D_loss_gp: 0.0741, G_loss: -687.3230\n",
            "Step [119200/200000], D_loss_real: -316.1457, D_loss_fake: 319.1005, D_loss_gp: 0.1611, G_loss: -335.2412\n",
            "Step [119300/200000], D_loss_real: -1602.3904, D_loss_fake: 1603.8643, D_loss_gp: 0.0284, G_loss: -1526.9751\n",
            "Step [119400/200000], D_loss_real: -1682.3362, D_loss_fake: 1683.1587, D_loss_gp: 0.0405, G_loss: -1668.1039\n",
            "Step [119500/200000], D_loss_real: -2048.8789, D_loss_fake: 2049.0454, D_loss_gp: 0.0415, G_loss: -2101.1321\n",
            "Step [119600/200000], D_loss_real: -2154.1958, D_loss_fake: 2148.8066, D_loss_gp: 0.1266, G_loss: -2227.2786\n",
            "Step [119700/200000], D_loss_real: -705.4648, D_loss_fake: 706.7447, D_loss_gp: 0.0278, G_loss: -676.5126\n",
            "Step [119800/200000], D_loss_real: -957.5482, D_loss_fake: 961.4366, D_loss_gp: 0.1337, G_loss: -950.1471\n",
            "Step [119900/200000], D_loss_real: -1358.4341, D_loss_fake: 1354.2123, D_loss_gp: 0.0821, G_loss: -1428.3048\n",
            "Step [120000/200000], D_loss_real: -896.0848, D_loss_fake: 896.5532, D_loss_gp: 0.0779, G_loss: -810.1776\n",
            "Step [120100/200000], D_loss_real: -1369.2812, D_loss_fake: 1374.0278, D_loss_gp: 0.0206, G_loss: -1346.4923\n",
            "Step [120200/200000], D_loss_real: -842.8346, D_loss_fake: 840.7477, D_loss_gp: 0.0434, G_loss: -817.0583\n",
            "Step [120300/200000], D_loss_real: -458.5757, D_loss_fake: 454.9190, D_loss_gp: 0.0326, G_loss: -517.4479\n",
            "Step [120400/200000], D_loss_real: 119.5270, D_loss_fake: -121.2114, D_loss_gp: 0.3620, G_loss: 103.4118\n",
            "Step [120500/200000], D_loss_real: -954.1352, D_loss_fake: 950.4684, D_loss_gp: 0.1063, G_loss: -980.0101\n",
            "Step [120600/200000], D_loss_real: -858.6823, D_loss_fake: 855.7211, D_loss_gp: 0.2149, G_loss: -781.1624\n",
            "Step [120700/200000], D_loss_real: -1109.6173, D_loss_fake: 1108.8745, D_loss_gp: 0.0263, G_loss: -1128.4819\n",
            "Step [120800/200000], D_loss_real: -1615.6399, D_loss_fake: 1603.4954, D_loss_gp: 0.0966, G_loss: -1699.0691\n",
            "Step [120900/200000], D_loss_real: -1671.3210, D_loss_fake: 1676.5103, D_loss_gp: 0.0218, G_loss: -1659.9417\n",
            "Step [121000/200000], D_loss_real: -1745.4774, D_loss_fake: 1748.0643, D_loss_gp: 0.0125, G_loss: -1716.5392\n",
            "Step [121100/200000], D_loss_real: -2478.9724, D_loss_fake: 2478.5977, D_loss_gp: 0.0346, G_loss: -2485.7637\n",
            "Step [121200/200000], D_loss_real: -3006.2827, D_loss_fake: 3011.1948, D_loss_gp: 0.0255, G_loss: -2938.0183\n",
            "Step [121300/200000], D_loss_real: -2961.1165, D_loss_fake: 2953.4326, D_loss_gp: 0.0393, G_loss: -3037.0786\n",
            "Step [121400/200000], D_loss_real: -2685.7727, D_loss_fake: 2686.7075, D_loss_gp: 0.0217, G_loss: -2683.7075\n",
            "Step [121500/200000], D_loss_real: -2664.7305, D_loss_fake: 2664.1162, D_loss_gp: 0.0044, G_loss: -2657.6455\n",
            "Step [121600/200000], D_loss_real: -2580.6152, D_loss_fake: 2568.6018, D_loss_gp: 0.0299, G_loss: -2656.4565\n",
            "Step [121700/200000], D_loss_real: -2016.1006, D_loss_fake: 2021.0942, D_loss_gp: 0.0508, G_loss: -2000.7856\n",
            "Step [121800/200000], D_loss_real: -1139.2241, D_loss_fake: 1133.2847, D_loss_gp: 0.0243, G_loss: -1132.7693\n",
            "Step [121900/200000], D_loss_real: -1351.0544, D_loss_fake: 1353.9746, D_loss_gp: 0.0417, G_loss: -1379.4832\n",
            "Step [122000/200000], D_loss_real: -1261.4435, D_loss_fake: 1263.5176, D_loss_gp: 0.1055, G_loss: -1234.9658\n",
            "Step [122100/200000], D_loss_real: -1352.0266, D_loss_fake: 1350.7483, D_loss_gp: 0.0173, G_loss: -1345.2233\n",
            "Step [122200/200000], D_loss_real: -995.8960, D_loss_fake: 995.9641, D_loss_gp: 0.0097, G_loss: -982.2542\n",
            "Step [122300/200000], D_loss_real: -1064.4349, D_loss_fake: 1065.0906, D_loss_gp: 0.0193, G_loss: -1058.3601\n",
            "Step [122400/200000], D_loss_real: -1192.4226, D_loss_fake: 1193.3694, D_loss_gp: 0.0226, G_loss: -1223.4124\n",
            "Step [122500/200000], D_loss_real: -1382.4548, D_loss_fake: 1372.9231, D_loss_gp: 0.0688, G_loss: -1383.8920\n",
            "Step [122600/200000], D_loss_real: -1227.7140, D_loss_fake: 1228.0035, D_loss_gp: 0.0184, G_loss: -1249.1377\n",
            "Step [122700/200000], D_loss_real: -1034.2120, D_loss_fake: 1029.0133, D_loss_gp: 0.0378, G_loss: -1000.0416\n",
            "Step [122800/200000], D_loss_real: -1184.0483, D_loss_fake: 1179.5566, D_loss_gp: 0.0223, G_loss: -1185.8169\n",
            "Step [122900/200000], D_loss_real: -1222.0781, D_loss_fake: 1220.5125, D_loss_gp: 0.0293, G_loss: -1210.6821\n",
            "Step [123000/200000], D_loss_real: -991.1874, D_loss_fake: 985.5042, D_loss_gp: 0.0060, G_loss: -968.0345\n",
            "Step [123100/200000], D_loss_real: -961.4835, D_loss_fake: 963.3035, D_loss_gp: 0.0822, G_loss: -954.5269\n",
            "Step [123200/200000], D_loss_real: -886.8063, D_loss_fake: 889.6638, D_loss_gp: 0.0344, G_loss: -889.8223\n",
            "Step [123300/200000], D_loss_real: -774.5503, D_loss_fake: 775.5086, D_loss_gp: 0.2095, G_loss: -787.0613\n",
            "Step [123400/200000], D_loss_real: -401.3705, D_loss_fake: 404.3483, D_loss_gp: 0.0956, G_loss: -398.2887\n",
            "Step [123500/200000], D_loss_real: -726.7800, D_loss_fake: 729.9168, D_loss_gp: 0.0303, G_loss: -726.5837\n",
            "Step [123600/200000], D_loss_real: -671.8488, D_loss_fake: 665.9503, D_loss_gp: 0.0092, G_loss: -651.4224\n",
            "Step [123700/200000], D_loss_real: -611.8529, D_loss_fake: 611.2670, D_loss_gp: 0.0182, G_loss: -612.1390\n",
            "Step [123800/200000], D_loss_real: -863.3464, D_loss_fake: 862.1683, D_loss_gp: 0.0077, G_loss: -872.1594\n",
            "Step [123900/200000], D_loss_real: -423.2906, D_loss_fake: 427.9631, D_loss_gp: 0.0471, G_loss: -440.8011\n",
            "Step [124000/200000], D_loss_real: -629.1967, D_loss_fake: 628.3796, D_loss_gp: 0.0195, G_loss: -634.6097\n",
            "Step [124100/200000], D_loss_real: -741.4769, D_loss_fake: 740.7351, D_loss_gp: 0.0105, G_loss: -760.1034\n",
            "Step [124200/200000], D_loss_real: -1183.3031, D_loss_fake: 1176.2080, D_loss_gp: 0.0216, G_loss: -1202.4243\n",
            "Step [124300/200000], D_loss_real: -877.5146, D_loss_fake: 874.3073, D_loss_gp: 0.0190, G_loss: -868.0683\n",
            "Step [124400/200000], D_loss_real: -848.1019, D_loss_fake: 846.5319, D_loss_gp: 0.0253, G_loss: -861.5466\n",
            "Step [124500/200000], D_loss_real: -915.4863, D_loss_fake: 919.8552, D_loss_gp: 0.0603, G_loss: -895.6493\n",
            "Step [124600/200000], D_loss_real: -910.1528, D_loss_fake: 908.7906, D_loss_gp: 0.0156, G_loss: -922.7161\n",
            "Step [124700/200000], D_loss_real: -1028.4099, D_loss_fake: 1029.3253, D_loss_gp: 0.0120, G_loss: -1005.5827\n",
            "Step [124800/200000], D_loss_real: -893.6047, D_loss_fake: 893.0654, D_loss_gp: 0.0702, G_loss: -893.4258\n",
            "Step [124900/200000], D_loss_real: -758.5558, D_loss_fake: 758.4604, D_loss_gp: 0.0141, G_loss: -757.5369\n",
            "Step [125000/200000], D_loss_real: -771.8528, D_loss_fake: 770.9487, D_loss_gp: 0.0209, G_loss: -789.6401\n",
            "Step [125100/200000], D_loss_real: -929.3553, D_loss_fake: 928.2474, D_loss_gp: 0.0580, G_loss: -927.8195\n",
            "Step [125200/200000], D_loss_real: -1035.9735, D_loss_fake: 1039.5208, D_loss_gp: 0.0408, G_loss: -1018.2776\n",
            "Step [125300/200000], D_loss_real: -929.3171, D_loss_fake: 927.2368, D_loss_gp: 0.0200, G_loss: -945.7093\n",
            "Step [125400/200000], D_loss_real: -1192.1760, D_loss_fake: 1192.7931, D_loss_gp: 0.0107, G_loss: -1198.5422\n",
            "Step [125500/200000], D_loss_real: -1614.6068, D_loss_fake: 1617.9985, D_loss_gp: 0.0706, G_loss: -1607.2034\n",
            "Step [125600/200000], D_loss_real: -1535.8977, D_loss_fake: 1540.1544, D_loss_gp: 0.0140, G_loss: -1525.1658\n",
            "Step [125700/200000], D_loss_real: -1764.1877, D_loss_fake: 1761.8298, D_loss_gp: 0.0374, G_loss: -1765.2661\n",
            "Step [125800/200000], D_loss_real: -1912.9551, D_loss_fake: 1914.7156, D_loss_gp: 0.0159, G_loss: -1911.1111\n",
            "Step [125900/200000], D_loss_real: -2036.0413, D_loss_fake: 2039.9331, D_loss_gp: 0.0851, G_loss: -2037.7871\n",
            "Step [126000/200000], D_loss_real: -1945.5829, D_loss_fake: 1934.0723, D_loss_gp: 0.0124, G_loss: -1982.3600\n",
            "Step [126100/200000], D_loss_real: -1952.2524, D_loss_fake: 1940.8799, D_loss_gp: 0.0124, G_loss: -1976.8962\n",
            "Step [126200/200000], D_loss_real: -2091.2026, D_loss_fake: 2082.0195, D_loss_gp: 0.0257, G_loss: -2135.8596\n",
            "Step [126300/200000], D_loss_real: -2298.2458, D_loss_fake: 2294.8047, D_loss_gp: 0.0201, G_loss: -2318.6729\n",
            "Step [126400/200000], D_loss_real: -2297.3149, D_loss_fake: 2296.9419, D_loss_gp: 0.0260, G_loss: -2300.8645\n",
            "Step [126500/200000], D_loss_real: -2279.5396, D_loss_fake: 2283.6138, D_loss_gp: 0.0610, G_loss: -2250.9746\n",
            "Step [126600/200000], D_loss_real: -2180.9194, D_loss_fake: 2184.8591, D_loss_gp: 0.0265, G_loss: -2144.9619\n",
            "Step [126700/200000], D_loss_real: -2228.7048, D_loss_fake: 2230.3120, D_loss_gp: 0.0695, G_loss: -2235.0959\n",
            "Step [126800/200000], D_loss_real: -2414.7800, D_loss_fake: 2416.5278, D_loss_gp: 0.0614, G_loss: -2409.9915\n",
            "Step [126900/200000], D_loss_real: -2453.7563, D_loss_fake: 2457.7251, D_loss_gp: 0.0245, G_loss: -2448.2759\n",
            "Step [127000/200000], D_loss_real: -2473.8867, D_loss_fake: 2468.5227, D_loss_gp: 0.0434, G_loss: -2490.2844\n",
            "Step [127100/200000], D_loss_real: -2471.3538, D_loss_fake: 2470.8628, D_loss_gp: 0.0272, G_loss: -2456.0845\n",
            "Step [127200/200000], D_loss_real: -2643.5889, D_loss_fake: 2641.8284, D_loss_gp: 0.0186, G_loss: -2651.7505\n",
            "Step [127300/200000], D_loss_real: -2238.1646, D_loss_fake: 2239.9204, D_loss_gp: 0.0098, G_loss: -2216.4939\n",
            "Step [127400/200000], D_loss_real: -2034.3582, D_loss_fake: 2037.1287, D_loss_gp: 0.0481, G_loss: -2025.0343\n",
            "Step [127500/200000], D_loss_real: -1900.9966, D_loss_fake: 1897.9280, D_loss_gp: 0.0118, G_loss: -1899.2263\n",
            "Step [127600/200000], D_loss_real: -1738.8702, D_loss_fake: 1731.4774, D_loss_gp: 0.0510, G_loss: -1768.4182\n",
            "Step [127700/200000], D_loss_real: -1600.4995, D_loss_fake: 1593.7251, D_loss_gp: 0.0776, G_loss: -1597.8318\n",
            "Step [127800/200000], D_loss_real: -1199.1411, D_loss_fake: 1201.9910, D_loss_gp: 0.0113, G_loss: -1185.2310\n",
            "Step [127900/200000], D_loss_real: -1283.8254, D_loss_fake: 1277.4978, D_loss_gp: 0.0095, G_loss: -1284.4238\n",
            "Step [128000/200000], D_loss_real: -1112.0267, D_loss_fake: 1107.4485, D_loss_gp: 0.0293, G_loss: -1097.9086\n",
            "Step [128100/200000], D_loss_real: -1224.2898, D_loss_fake: 1224.2806, D_loss_gp: 0.0087, G_loss: -1208.5311\n",
            "Step [128200/200000], D_loss_real: -929.0889, D_loss_fake: 926.7023, D_loss_gp: 0.0068, G_loss: -924.6205\n",
            "Step [128300/200000], D_loss_real: -978.6902, D_loss_fake: 977.8690, D_loss_gp: 0.0072, G_loss: -981.2783\n",
            "Step [128400/200000], D_loss_real: -997.0732, D_loss_fake: 999.0706, D_loss_gp: 0.0114, G_loss: -997.5456\n",
            "Step [128500/200000], D_loss_real: -968.9552, D_loss_fake: 967.1320, D_loss_gp: 0.0067, G_loss: -979.5913\n",
            "Step [128600/200000], D_loss_real: -991.4950, D_loss_fake: 998.4641, D_loss_gp: 0.0165, G_loss: -1004.9735\n",
            "Step [128700/200000], D_loss_real: -1082.1449, D_loss_fake: 1083.8011, D_loss_gp: 0.0210, G_loss: -1077.8912\n",
            "Step [128800/200000], D_loss_real: -1068.7771, D_loss_fake: 1068.0044, D_loss_gp: 0.0689, G_loss: -1053.3510\n",
            "Step [128900/200000], D_loss_real: -1074.3181, D_loss_fake: 1075.5590, D_loss_gp: 0.0179, G_loss: -1064.9722\n",
            "Step [129000/200000], D_loss_real: -913.0941, D_loss_fake: 915.3512, D_loss_gp: 0.0042, G_loss: -896.9556\n",
            "Step [129100/200000], D_loss_real: -934.3146, D_loss_fake: 934.3071, D_loss_gp: 0.0083, G_loss: -933.4772\n",
            "Step [129200/200000], D_loss_real: -933.5167, D_loss_fake: 936.8976, D_loss_gp: 0.0496, G_loss: -927.2795\n",
            "Step [129300/200000], D_loss_real: -887.8752, D_loss_fake: 887.1359, D_loss_gp: 0.0849, G_loss: -850.6763\n",
            "Step [129400/200000], D_loss_real: -772.4686, D_loss_fake: 766.8632, D_loss_gp: 0.3951, G_loss: -773.2331\n",
            "Step [129500/200000], D_loss_real: -863.0343, D_loss_fake: 855.2228, D_loss_gp: 0.0259, G_loss: -848.1769\n",
            "Step [129600/200000], D_loss_real: -654.4368, D_loss_fake: 660.6048, D_loss_gp: 0.0489, G_loss: -638.8652\n",
            "Step [129700/200000], D_loss_real: -564.9404, D_loss_fake: 570.0477, D_loss_gp: 0.1759, G_loss: -534.2398\n",
            "Step [129800/200000], D_loss_real: -750.1400, D_loss_fake: 748.6726, D_loss_gp: 0.1910, G_loss: -758.3813\n",
            "Step [129900/200000], D_loss_real: -1027.3406, D_loss_fake: 1022.2302, D_loss_gp: 0.0652, G_loss: -1019.0605\n",
            "Step [130000/200000], D_loss_real: -829.5102, D_loss_fake: 833.0978, D_loss_gp: 0.0573, G_loss: -813.4510\n",
            "Step [130100/200000], D_loss_real: -641.1761, D_loss_fake: 641.8765, D_loss_gp: 0.0441, G_loss: -651.8670\n",
            "Step [130200/200000], D_loss_real: -306.8106, D_loss_fake: 300.3851, D_loss_gp: 0.0768, G_loss: -328.8753\n",
            "Step [130300/200000], D_loss_real: -352.4301, D_loss_fake: 359.6512, D_loss_gp: 0.1163, G_loss: -378.3386\n",
            "Step [130400/200000], D_loss_real: -515.9202, D_loss_fake: 516.5508, D_loss_gp: 0.0731, G_loss: -525.5192\n",
            "Step [130500/200000], D_loss_real: -476.1494, D_loss_fake: 475.5229, D_loss_gp: 0.0290, G_loss: -464.4186\n",
            "Step [130600/200000], D_loss_real: -636.5136, D_loss_fake: 633.9771, D_loss_gp: 0.1875, G_loss: -654.0825\n",
            "Step [130700/200000], D_loss_real: -731.3945, D_loss_fake: 734.8085, D_loss_gp: 0.0383, G_loss: -718.6325\n",
            "Step [130800/200000], D_loss_real: -364.1145, D_loss_fake: 364.4567, D_loss_gp: 0.0892, G_loss: -362.5928\n",
            "Step [130900/200000], D_loss_real: -298.8536, D_loss_fake: 296.4973, D_loss_gp: 0.0079, G_loss: -305.5203\n",
            "Step [131000/200000], D_loss_real: -155.7611, D_loss_fake: 156.0260, D_loss_gp: 0.0361, G_loss: -152.4547\n",
            "Step [131100/200000], D_loss_real: -522.6454, D_loss_fake: 525.4672, D_loss_gp: 0.1236, G_loss: -526.9594\n",
            "Step [131200/200000], D_loss_real: -673.7345, D_loss_fake: 676.3303, D_loss_gp: 0.0258, G_loss: -676.0341\n",
            "Step [131300/200000], D_loss_real: -535.9711, D_loss_fake: 537.7256, D_loss_gp: 0.0156, G_loss: -532.6108\n",
            "Step [131400/200000], D_loss_real: -581.9265, D_loss_fake: 580.5270, D_loss_gp: 0.0180, G_loss: -588.9158\n",
            "Step [131500/200000], D_loss_real: -627.9935, D_loss_fake: 625.3635, D_loss_gp: 0.0149, G_loss: -639.2367\n",
            "Step [131600/200000], D_loss_real: -640.0833, D_loss_fake: 643.3184, D_loss_gp: 0.0126, G_loss: -640.1217\n",
            "Step [131700/200000], D_loss_real: -593.6487, D_loss_fake: 590.9599, D_loss_gp: 0.0440, G_loss: -586.2103\n",
            "Step [131800/200000], D_loss_real: -592.7449, D_loss_fake: 602.8330, D_loss_gp: 0.0232, G_loss: -577.8623\n",
            "Step [131900/200000], D_loss_real: -279.7477, D_loss_fake: 287.0983, D_loss_gp: 0.6382, G_loss: -288.7655\n",
            "Step [132000/200000], D_loss_real: -517.7189, D_loss_fake: 519.2155, D_loss_gp: 0.0276, G_loss: -510.9116\n",
            "Step [132100/200000], D_loss_real: -83.4141, D_loss_fake: 84.5106, D_loss_gp: 0.6744, G_loss: -71.8533\n",
            "Step [132200/200000], D_loss_real: -420.2901, D_loss_fake: 427.6873, D_loss_gp: 0.1041, G_loss: -412.4557\n",
            "Step [132300/200000], D_loss_real: 161.5987, D_loss_fake: -158.4549, D_loss_gp: 0.6000, G_loss: 163.1884\n",
            "Step [132400/200000], D_loss_real: 192.2276, D_loss_fake: -187.1804, D_loss_gp: 0.1423, G_loss: 160.1245\n",
            "Step [132500/200000], D_loss_real: -303.7734, D_loss_fake: 311.8542, D_loss_gp: 0.1018, G_loss: -303.6734\n",
            "Step [132600/200000], D_loss_real: -223.6614, D_loss_fake: 228.6914, D_loss_gp: 0.3273, G_loss: -226.0702\n",
            "Step [132700/200000], D_loss_real: -132.2823, D_loss_fake: 136.6877, D_loss_gp: 0.4348, G_loss: -139.9243\n",
            "Step [132800/200000], D_loss_real: 84.3404, D_loss_fake: -84.3979, D_loss_gp: 0.8370, G_loss: 83.5928\n",
            "Step [132900/200000], D_loss_real: 44.9202, D_loss_fake: -30.6129, D_loss_gp: 0.9836, G_loss: 30.4271\n",
            "Step [133000/200000], D_loss_real: 156.3077, D_loss_fake: -154.9102, D_loss_gp: 2.0108, G_loss: 131.0724\n",
            "Step [133100/200000], D_loss_real: -315.1133, D_loss_fake: 309.2047, D_loss_gp: 0.0029, G_loss: -319.3600\n",
            "Step [133200/200000], D_loss_real: -92.4314, D_loss_fake: 99.9999, D_loss_gp: 0.1367, G_loss: -95.2986\n",
            "Step [133300/200000], D_loss_real: 169.7865, D_loss_fake: -165.5753, D_loss_gp: 0.0491, G_loss: 162.0085\n",
            "Step [133400/200000], D_loss_real: 128.4963, D_loss_fake: -127.1239, D_loss_gp: 0.1398, G_loss: 130.4510\n",
            "Step [133500/200000], D_loss_real: 40.4010, D_loss_fake: -37.8607, D_loss_gp: 0.5280, G_loss: 36.2559\n",
            "Step [133600/200000], D_loss_real: -74.1640, D_loss_fake: 76.3892, D_loss_gp: 0.0996, G_loss: -73.5244\n",
            "Step [133700/200000], D_loss_real: -35.8788, D_loss_fake: 46.9540, D_loss_gp: 0.0303, G_loss: -35.4583\n",
            "Step [133800/200000], D_loss_real: 122.7099, D_loss_fake: -119.1360, D_loss_gp: 0.0576, G_loss: 126.2709\n",
            "Step [133900/200000], D_loss_real: 158.7247, D_loss_fake: -155.6764, D_loss_gp: 0.2203, G_loss: 157.6225\n",
            "Step [134000/200000], D_loss_real: 69.4098, D_loss_fake: -69.0530, D_loss_gp: 0.2904, G_loss: 70.5474\n",
            "Step [134100/200000], D_loss_real: 0.0880, D_loss_fake: 14.5447, D_loss_gp: 0.1685, G_loss: -11.5659\n",
            "Step [134200/200000], D_loss_real: -118.4710, D_loss_fake: 123.7655, D_loss_gp: 0.2934, G_loss: -126.8860\n",
            "Step [134300/200000], D_loss_real: 55.3226, D_loss_fake: -59.5268, D_loss_gp: 0.5574, G_loss: 54.0219\n",
            "Step [134400/200000], D_loss_real: 212.3109, D_loss_fake: -200.4972, D_loss_gp: 0.0468, G_loss: 195.7610\n",
            "Step [134500/200000], D_loss_real: 72.8787, D_loss_fake: -74.8067, D_loss_gp: 0.6705, G_loss: 81.3177\n",
            "Step [134600/200000], D_loss_real: 29.0931, D_loss_fake: -22.2237, D_loss_gp: 0.2809, G_loss: 24.1407\n",
            "Step [134700/200000], D_loss_real: 79.4559, D_loss_fake: -70.4236, D_loss_gp: 0.4557, G_loss: 66.6951\n",
            "Step [134800/200000], D_loss_real: 160.2159, D_loss_fake: -155.2160, D_loss_gp: 0.3526, G_loss: 156.1965\n",
            "Step [134900/200000], D_loss_real: 111.5932, D_loss_fake: -108.0646, D_loss_gp: 0.6238, G_loss: 106.2389\n",
            "Step [135000/200000], D_loss_real: 98.4996, D_loss_fake: -97.9721, D_loss_gp: 0.6188, G_loss: 96.9739\n",
            "Step [135100/200000], D_loss_real: 124.8768, D_loss_fake: -123.9886, D_loss_gp: 0.4616, G_loss: 122.2036\n",
            "Step [135200/200000], D_loss_real: 93.0433, D_loss_fake: -92.3434, D_loss_gp: 0.5847, G_loss: 87.6263\n",
            "Step [135300/200000], D_loss_real: 53.8817, D_loss_fake: -50.1487, D_loss_gp: 0.1827, G_loss: 53.0666\n",
            "Step [135400/200000], D_loss_real: 136.0760, D_loss_fake: -134.8674, D_loss_gp: 0.5873, G_loss: 135.9975\n",
            "Step [135500/200000], D_loss_real: 130.9898, D_loss_fake: -128.4444, D_loss_gp: 0.6456, G_loss: 129.5539\n",
            "Step [135600/200000], D_loss_real: 105.9699, D_loss_fake: -103.8370, D_loss_gp: 0.6322, G_loss: 103.0835\n",
            "Step [135700/200000], D_loss_real: 123.2692, D_loss_fake: -122.0426, D_loss_gp: 0.7714, G_loss: 125.2072\n",
            "Step [135800/200000], D_loss_real: 96.4630, D_loss_fake: -96.3808, D_loss_gp: 0.6552, G_loss: 94.9956\n",
            "Step [135900/200000], D_loss_real: 107.7364, D_loss_fake: -105.4198, D_loss_gp: 0.7662, G_loss: 106.4009\n",
            "Step [136000/200000], D_loss_real: 126.1155, D_loss_fake: -124.2610, D_loss_gp: 0.7223, G_loss: 123.0602\n",
            "Step [136100/200000], D_loss_real: 110.6498, D_loss_fake: -109.9420, D_loss_gp: 0.7677, G_loss: 110.0093\n",
            "Step [136200/200000], D_loss_real: 70.1130, D_loss_fake: -70.5983, D_loss_gp: 0.3661, G_loss: 68.9615\n",
            "Step [136300/200000], D_loss_real: 150.0554, D_loss_fake: -144.4794, D_loss_gp: 0.6339, G_loss: 144.2535\n",
            "Step [136400/200000], D_loss_real: 98.5588, D_loss_fake: -99.6837, D_loss_gp: 0.5024, G_loss: 96.2597\n",
            "Step [136500/200000], D_loss_real: 155.5383, D_loss_fake: -158.5041, D_loss_gp: 0.1679, G_loss: 157.5374\n",
            "Step [136600/200000], D_loss_real: 150.6567, D_loss_fake: -151.8138, D_loss_gp: 0.8002, G_loss: 158.1045\n",
            "Step [136700/200000], D_loss_real: 61.4891, D_loss_fake: -67.9886, D_loss_gp: 0.0973, G_loss: 65.5184\n",
            "Step [136800/200000], D_loss_real: 133.4398, D_loss_fake: -143.4958, D_loss_gp: 0.5786, G_loss: 146.8831\n",
            "Step [136900/200000], D_loss_real: 109.9607, D_loss_fake: -102.7502, D_loss_gp: 0.4306, G_loss: 103.3543\n",
            "Step [137000/200000], D_loss_real: 123.8375, D_loss_fake: -120.6501, D_loss_gp: 1.8609, G_loss: 119.9392\n",
            "Step [137100/200000], D_loss_real: 106.4023, D_loss_fake: -103.3771, D_loss_gp: 0.6153, G_loss: 103.3677\n",
            "Step [137200/200000], D_loss_real: 125.1138, D_loss_fake: -119.7269, D_loss_gp: 0.5741, G_loss: 119.9710\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-12-3989665991.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;31m# Discriminator output for interpolated images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m     \u001b[0md_out_interpolated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterpolated_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_labels_int\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;31m# Compute gradients of D_out_interpolated with respect to interpolated_images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-8-3668333695.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image, class_id_int)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_id_int\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m# Initial block processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0;31m# Add skip connection for initial block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_skip_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-5-1116004030.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_uv_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mAdaptiveNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Plot Generator and Discriminator losses**"
      ],
      "metadata": {
        "id": "8OwIy5pNMkWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SAMPLE_OUTPUT_DIR = os.path.join('/content/samples', 'Gan_flower102')\n",
        "\n",
        "# --- Plot 1: Discriminator Total Loss ---\n",
        "plot_path_d = os.path.join(SAMPLE_OUTPUT_DIR, 'discriminator_total_loss.png')\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(steps_history, d_total_loss_history, label='Discriminator Total Loss', color='blue', alpha=0.8)\n",
        "plt.xlabel('Training Step')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Discriminator Total Loss Over Time')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(plot_path_d)\n",
        "plt.show()\n",
        "print(f\"Discriminator loss plot saved to: {plot_path_d}\")\n",
        "\n",
        "# --- Plot 2: Generator Loss ---\n",
        "plot_path_g = os.path.join(SAMPLE_OUTPUT_DIR, 'generator_loss.png')\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(steps_history, g_total_loss_history, label='Generator Loss', color='red', alpha=0.8)\n",
        "plt.xlabel('Training Step')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Generator Loss Over Time')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(plot_path_g)\n",
        "plt.show()\n",
        "print(f\"Generator loss plot saved to: {plot_path_g}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "22dj35o4MiWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Animation of Training Progress**"
      ],
      "metadata": {
        "id": "C4PtAvnxsxAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_animation(sample_dir, output_gif_name=\"training_progress.gif\", fps=10):\n",
        "    image_files = []\n",
        "    sorted_files = sorted(\n",
        "        [f for f in os.listdir(sample_dir) if f.endswith('_generated.png')],\n",
        "        key=lambda x: int(re.findall(r'(\\d+)_generated\\.png', x)[0]) if re.findall(r'(\\d+)_generated\\.png', x) else 0\n",
        "    )\n",
        "\n",
        "    for f in sorted_files:\n",
        "        image_files.append(os.path.join(sample_dir, f))\n",
        "\n",
        "    images_for_gif = []\n",
        "    for image_file in image_files:\n",
        "      img = Image.open(image_file).convert('RGB')\n",
        "      images_for_gif.append(np.array(img))\n",
        "\n",
        "    fps = 10\n",
        "\n",
        "    output_path = os.path.join(sample_dir, output_gif_name)\n",
        "\n",
        "    imageio.mimsave(output_path, images_for_gif, fps=fps)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(images_for_gif[0])\n",
        "    ax.axis('off')\n",
        "    plt.title('GAN Training Progress Animation')\n",
        "\n",
        "    def update(frame):\n",
        "        im.set_array(images_for_gif[frame])\n",
        "        return [im]\n",
        "\n",
        "    ani = animation.FuncAnimation(\n",
        "        fig, update, frames=len(images_for_gif),\n",
        "        interval=1000 / fps, blit=True, repeat=False\n",
        "    )\n",
        "\n",
        "    plt.show(block=False)\n",
        "\n",
        "create_animation(SAMPLE_OUTPUT_DIR)\n"
      ],
      "metadata": {
        "id": "4jy6V9Xl3VCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Plot Real Images vs Fake Images**"
      ],
      "metadata": {
        "id": "BRDgk0THs9V8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_real_vs_fake(real_image, generated_image):\n",
        "    real_img = Image.open(real_image)\n",
        "    final_fake_img = Image.open(generated_image)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "    axes[0].imshow(real_img)\n",
        "    axes[0].set_title('Sample Real Images')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    axes[1].imshow(final_fake_img)\n",
        "    axes[1].set_title('Final Generated Images')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    plt.suptitle('Real vs. Generated Images After Training', fontsize=16)\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    output_comparison_path = os.path.join(SAMPLE_OUTPUT_DIR, 'real_vs_fake_comparison.png')\n",
        "    plt.savefig(output_comparison_path)\n",
        "    plt.show()\n",
        "\n",
        "final_step_str = str(100000)\n",
        "generated_image_sample = os.path.join(SAMPLE_OUTPUT_DIR, f'{final_step_str}_generated.png')\n",
        "real_image_sample = os.path.join(SAMPLE_OUTPUT_DIR, 'real_images_sample.png')\n",
        "\n",
        "display_real_vs_fake(real_image=real_image_sample, generated_image=generated_image_sample)\n"
      ],
      "metadata": {
        "id": "YpBUWaFDVL9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Ev9D4LJjypL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}